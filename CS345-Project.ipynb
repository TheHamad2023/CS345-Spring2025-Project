{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4315aa3e",
   "metadata": {},
   "source": [
    "# CS345 Project\n",
    "\n",
    "## Team Members\n",
    "1. Hamad Alyami\n",
    "2. Benito Encarnacion\n",
    "\n",
    "## Dataset\n",
    "Our dataset was from Kaggle by a user called Mexwell. The data is paragraphs scraped from wikipedia in 2018 in 235 languages.\n",
    "\n",
    "The dataset contains 235,000 datasets with balance between language proportions and a test and train split provided.\n",
    "\n",
    "The downloaded folder from Kaggle contains:\n",
    "- labels.csv: A file containing the language name, 2-3 letter code, German name, and language family of all the languages present in the dataset.\n",
    "- README.txt: A file explaining the folder contents.\n",
    "- urls.txt: A file containing the urls of where the paragraphs were found.\n",
    "- x_test.txt: The testing data samples, paragraphs in multiple languages.\n",
    "- x_train.txt: The training data samples, paragraphs in multiple langauges\n",
    "- y_test.txt: The labels for the testing dataset, using the 2-3 letter codes found in labels.csv.\n",
    "- y_train.txt: The labels for the training dataset, using the 2-3 letter codes found in labels.csv\n",
    "\n",
    "\n",
    "## Project\n",
    "Our project is to train and compare two ML models on the Latin Alphabet languages present in the dataset and compare their performance.\n",
    "\n",
    "## Motivation\n",
    "We decided to do this project because it allows us to explore practical applications of natural language processing and machine learning by working with real-world multilingual data. Language identification is an important task in many systems and applications like search engines, translation tools, and content moderation. Working with such a dataset gives us the opportunity to apply classification techniques in a meaningful way. By focusing on languages that use the Latin alphabet, we avoid complications from different writing systems while still working with a variety of languages.\n",
    "\n",
    "## Models\n",
    "The models we decided to work with in this project are:\n",
    "- Multinomial Naive-Bayes (MNB): Uses word frequencies in each class, langauges in our case, to guess the most likely class for text it has not seen.\n",
    "\n",
    "- Feed Forward Neural Network (FNN): An artificial Neural Network where information moves from input to output without looping back. It uses neurons, connected nodes, to learn patterns and make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1704ce66",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "We will begin by reading the data from the files then:\n",
    "1. Remove Null Values\n",
    "2. Filter to keep texts of languages we want using the 2-3 letter codes\n",
    "3. Return both samples from x_test and x_train and labels from y_test and y_train stacked into X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7602a789-a084-4f24-b977-47df8039c2a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data/x_train.txt: Read!\n",
      "Data/y_train.txt: Read!\n",
      "Data/x_test.txt: Read!\n",
      "Data/y_test.txt: Read!\n",
      "(10000,) (10000,)\n"
     ]
    }
   ],
   "source": [
    "#Understanding the data set\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def file_to_np_array(path, label):\n",
    "    try:\n",
    "        df = pd.read_csv(path, sep='<NonExistenceSeparator>', header=None, engine='python')\n",
    "        print(f\"{label}: Read!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading the {label} file: {e}\")\n",
    "        return None\n",
    "    return df.to_numpy()\n",
    "\n",
    "\n",
    "def clean_np_data(X, y):\n",
    "    stacked = np.hstack((y, X)) # Stack y and X side by side\n",
    "    # print(stacked.shape)\n",
    "    clean_stacked = stacked[~np.any(pd.isna(stacked), axis=1), :] # Remove empty values\n",
    "    # print(clean_stacked.shape)\n",
    "    lang_codes = ['ita', 'fra', 'spa', 'por', 'eng', 'deu', 'nld', 'ind', 'fin', 'hau']\n",
    "    true_clean = clean_stacked[np.isin(clean_stacked[:,0], lang_codes),:] # Remove all rows that aren't our target languages\n",
    "    # print(true_clean.shape)\n",
    "    return true_clean[:,1], true_clean[:,0] # Return cleaned as X and y split again\n",
    "\n",
    "def clean_filter_and_stack(X_train_file, y_train_file, X_test_file, y_test_file):\n",
    "    X_train_clean, y_train_clean = clean_np_data(file_to_np_array(X_train_file, X_train_file), \n",
    "                                       file_to_np_array(y_train_file, y_train_file))\n",
    "    X_test_clean, y_test_clean = clean_np_data(file_to_np_array(X_test_file, X_test_file), \n",
    "                                       file_to_np_array(y_test_file, y_test_file))\n",
    "    return np.hstack((X_train_clean, X_test_clean)), np.hstack((y_train_clean, y_test_clean))\n",
    "\n",
    "X, y = clean_filter_and_stack(\"Data/x_train.txt\", \n",
    "                                      \"Data/y_train.txt\", \n",
    "                                      \"Data/x_test.txt\", \n",
    "                                      \"Data/y_test.txt\")\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d27cb38f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>avg_word_count</th>\n",
       "      <th>percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>deu</td>\n",
       "      <td>59.762</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eng</td>\n",
       "      <td>70.455</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fin</td>\n",
       "      <td>48.431</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fra</td>\n",
       "      <td>67.707</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hau</td>\n",
       "      <td>75.802</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ind</td>\n",
       "      <td>57.147</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ita</td>\n",
       "      <td>68.192</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>nld</td>\n",
       "      <td>55.657</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>por</td>\n",
       "      <td>66.184</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>spa</td>\n",
       "      <td>67.295</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lang  avg_word_count  percentage\n",
       "0  deu          59.762        10.0\n",
       "1  eng          70.455        10.0\n",
       "2  fin          48.431        10.0\n",
       "3  fra          67.707        10.0\n",
       "4  hau          75.802        10.0\n",
       "5  ind          57.147        10.0\n",
       "6  ita          68.192        10.0\n",
       "7  nld          55.657        10.0\n",
       "8  por          66.184        10.0\n",
       "9  spa          67.295        10.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def summarize_language_data(X, y):\n",
    "    def count_words(text):\n",
    "        return len(str(text).split())\n",
    "\n",
    "    df = pd.DataFrame({'text': X.flatten(), 'lang': y.flatten()})\n",
    "    df['word_count'] = df['text'].apply(count_words)\n",
    "\n",
    "    total_count = len(df)\n",
    "    grouped = df.groupby('lang').agg(\n",
    "        entry_count=('lang', 'count'),\n",
    "        avg_word_count=('word_count', 'mean')\n",
    "    ).reset_index()\n",
    "\n",
    "    grouped['percentage'] = (grouped['entry_count'] / total_count) * 100\n",
    "    grouped = grouped.drop(columns='entry_count')\n",
    "\n",
    "    return grouped\n",
    "\n",
    "display(summarize_language_data(X, y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a7d528",
   "metadata": {},
   "source": [
    "#### Data Split\n",
    "Here we use Sklearn train_test_split to split our data into 70/30 train and test splits, respectively, after shuffling them randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "60b758fc-1a04-4cc4-8c94-9c6034e4ca00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7000,) (7000,)\n",
      "(3000,) (3000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=17)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a5925257-f51e-452b-be5b-7a4dede72735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done vectorizing\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(strip_accents='unicode')\n",
    "X_tr_vectors = vectorizer.fit_transform(X_train)\n",
    "X_te_vectors = vectorizer.transform(X_test)\n",
    "print(\"Done vectorizing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "eb703257-141d-403f-a819-31553f0ccde4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training MNB\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(X_tr_vectors, y_train)\n",
    "print(\"Done training MNB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b8e9320b-e9e6-4217-8dc6-e068913a34f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.981\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = model.predict(X_te_vectors)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "# print(y_pred[0:10])\n",
    "# print(X_test[0:10])\n",
    "\n",
    "# def group_accuracy(group):\n",
    "#     return accuracy_score(group['data'], group['pred'])\n",
    "\n",
    "# df = pd.DataFrame({'data': y_test, 'pred': y_pred})\n",
    "\n",
    "# lang_accuracy = df.groupby('data').apply(group_accuracy).reset_index(name='accuracy')\n",
    "\n",
    "# print(\"\\nAccuracy per language:\")\n",
    "# print(lang_accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d6603099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy per language:\n",
      "true_label\n",
      "deu    0.975694\n",
      "eng    1.000000\n",
      "fin    0.993151\n",
      "fra    0.993355\n",
      "hau    0.996622\n",
      "ind    0.986159\n",
      "ita    0.968553\n",
      "nld    0.982759\n",
      "por    0.937282\n",
      "spa    0.976471\n",
      "Name: correct, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({'true_label': y_test, 'pred': y_pred})\n",
    "df['correct'] = (df['true_label'] == df['pred']).astype(int)\n",
    "\n",
    "lang_accuracy = df.groupby('true_label')['correct'].mean()\n",
    "\n",
    "print(\"\\nAccuracy per language:\")\n",
    "print(lang_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2f966d3e-57e4-4b15-9897-32ca8d142f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Results (Accuracy):\n",
      "  Fold 1: 98.25%\n",
      "  Fold 2: 98.30%\n",
      "  Fold 3: 98.25%\n",
      "  Fold 4: 98.60%\n",
      "  Fold 5: 98.40%\n",
      "Mean Accuracy: 98.36%\n",
      "\n",
      "Mean Accuracy per Language Across All Folds:\n",
      "true\n",
      "deu    0.975694\n",
      "eng    1.000000\n",
      "fin    0.993151\n",
      "fra    0.993355\n",
      "hau    0.996622\n",
      "ind    0.986159\n",
      "ita    0.968553\n",
      "nld    0.982759\n",
      "por    0.937282\n",
      "spa    0.976471\n",
      "Name: correct, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "model = make_pipeline(CountVectorizer(), MultinomialNB())\n",
    "\n",
    "num_folds = 5\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=17)\n",
    "\n",
    "cross_val_results = cross_val_score(model, X.flatten(), y.flatten(), cv=kf)\n",
    "\n",
    "all_preds = []\n",
    "all_true = []\n",
    "\n",
    "print(\"Cross-Validation Results (Accuracy):\")\n",
    "for i, result in enumerate(cross_val_results, 1):\n",
    "    print(f\"  Fold {i}: {result * 100:.2f}%\")\n",
    "\n",
    "    all_preds.extend(y_pred)\n",
    "    all_true.extend(y_test)\n",
    "\n",
    "print(f'Mean Accuracy: {cross_val_results.mean() * 100:.2f}%')\n",
    "\n",
    "df = pd.DataFrame({'true': all_true, 'pred': all_preds})\n",
    "df['correct'] = (df['true'] == df['pred']).astype(int)\n",
    "\n",
    "# Group by language and compute mean accuracy across all folds\n",
    "lang_accuracy = df.groupby('true')['correct'].mean()\n",
    "\n",
    "print(\"\\nMean Accuracy per Language Across All Folds:\")\n",
    "print(lang_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "81a93297-6dd5-4cb8-af86-0ff9a0616fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorganizing data for Feed Forward Neural Network input\n",
    "\n",
    "# 1) convert string labels into integer labels\n",
    "#      and make func to convert back\n",
    "\n",
    "all_str_labels = ['ita', 'fra', 'spa', 'por', 'eng', 'deu', 'nld', 'ind', 'fin', 'hau']\n",
    "\n",
    "def str_labels_to_int_labels(labelArr, string_labels):\n",
    "    rtn = np.empty(labelArr.shape, dtype=int)\n",
    "    for i, v in enumerate(string_labels):\n",
    "        rtn[labelArr == v] = i\n",
    "    return rtn\n",
    "\n",
    "def int_labels_to_str_labels(labelArr, string_labels):\n",
    "    rtn = np.empty(labelArr.shape, dtype='object')\n",
    "    for i, v in enumerate(string_labels):\n",
    "        rtn[labelArr == i] = v\n",
    "    return rtn\n",
    "\n",
    "# print(y_test[0:5])\n",
    "# y1 = str_labels_to_int_labels(y_test, all_str_labels)\n",
    "# print(y1[0:5])\n",
    "# y2 = int_labels_to_str_labels(y1, all_str_labels)\n",
    "# print(y2[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "1be68002-eb66-461c-94b4-7fa37cc9e4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) convert data into multi-column matrix of characters\n",
    "#      and make func to convert back\n",
    "\n",
    "def str_vec_to_float_matrix(strVec, longest_str_len):\n",
    "    # Pad strings to all be equal length\n",
    "    padded_strVec = np.char.ljust(strVec, longest_str_len, fillchar=' ')\n",
    "\n",
    "    # turn vector of strings into matrix of characters\n",
    "    stacked_char_matrix = np.vstack([np.array(list(s)) for s in padded_strVec])\n",
    "\n",
    "    # turn char matrix into int matrix\n",
    "    char_matrix_to_int_matrix = np.vectorize(ord)\n",
    "    int_matrix = char_matrix_to_int_matrix(stacked_char_matrix)\n",
    "\n",
    "    #normalize and scale so each value is a float between 0 and 1\n",
    "    matrix_max = np.max(int_matrix)\n",
    "    matrix_min = np.min(int_matrix)\n",
    "    min_subtracted_matrix = int_matrix - matrix_min\n",
    "    normalized_matrix = (min_subtracted_matrix / (matrix_max - matrix_min))\n",
    "    return normalized_matrix\n",
    "\n",
    "# Don't need to convert matrices back into rows of text because the neural network isn't designed to generate anything, just classify\n",
    "# def int_matrix_to_str_vec(intMatrix):\n",
    "#     int_matrix_to_char_matrix = np.vectorize(chr)\n",
    "#     char_matrix = int_matrix_to_char_matrix(intMatrix)\n",
    "#     padded_strVec = np.array([\"\".join(r) for r in char_matrix])\n",
    "#     return np.char.rstrip(padded_strVec)\n",
    "\n",
    "# print(X_test[0])\n",
    "# X_x = str_vec_to_int_matrix(X_test.astype(str))\n",
    "# print(X_x[0])\n",
    "# X_y = int_matrix_to_str_vec(X_x)\n",
    "# print(X_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "58228e9e-a643-4b73-a51a-ba3366793109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done converting data into FFNN format\n"
     ]
    }
   ],
   "source": [
    "# 3) use them both\n",
    "\n",
    "def convert_to_FFNN_format(Xr, Xe, yr, ye):\n",
    "    all_string_labels = ['ita', 'fra', 'spa', 'por', 'eng', 'deu', 'nld', 'ind', 'fin', 'hau']\n",
    "    max_str_len_1 = np.max(np.char.str_len(Xr.astype(str)))\n",
    "    max_str_len_2 = np.max(np.char.str_len(Xe.astype(str)))\n",
    "    max_str_len = max(max_str_len_1, max_str_len_2)\n",
    "    Xr_rtn = str_vec_to_float_matrix(Xr.astype(str), max_str_len)\n",
    "    Xe_rtn = str_vec_to_float_matrix(Xe.astype(str), max_str_len)\n",
    "    return (Xr_rtn, \n",
    "            Xe_rtn,\n",
    "            str_labels_to_int_labels(yr, all_string_labels), \n",
    "            str_labels_to_int_labels(ye, all_string_labels))\n",
    "\n",
    "(X_tr_nn, X_te_nn, y_tr_nn, y_te_nn) = convert_to_FFNN_format(X_train, X_test, y_train, y_test)\n",
    "\n",
    "print(\"Done converting data into FFNN format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c436646e-e772-42a2-9b08-ba6d866aaecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7000, 5577)\n",
      "(3000, 5577)\n"
     ]
    }
   ],
   "source": [
    "# print(X_tr_nn[0])\n",
    "# print(X_tr_nn[1])\n",
    "print(X_tr_nn.shape)\n",
    "print(X_te_nn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "07b30fd7-52e8-4fec-8803-69c7e421ef36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done importing Tensorflow Stuff\n"
     ]
    }
   ],
   "source": [
    "# Applying properly structured data to a basic FFNN\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
    "\n",
    "print(\"Done importing Tensorflow Stuff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "f025e6da-c3bf-47bb-a146-ab488730faf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make FFNN\n",
    "\n",
    "sample_length = X_tr_nn[0].shape[0]\n",
    "\n",
    "FFNN_model = Sequential([\n",
    "    Dense(sample_length, activation='relu'),\n",
    "    Dense(sample_length, activation='relu'),\n",
    "    Dense(1024, activation='relu'),\n",
    "    Dense(1024, activation='relu'),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "FFNN_model.compile(optimizer=Adam(),\n",
    "                   loss=SparseCategoricalCrossentropy(), \n",
    "                   metrics=[SparseCategoricalAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "2a0f14b7-7541-4e8c-af62-cfb93a3dbe48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219/219 [==============================] - 44s 188ms/step - loss: 2.3032 - sparse_categorical_accuracy: 0.0959\n",
      "94/94 [==============================] - 3s 27ms/step - loss: 2.3033 - sparse_categorical_accuracy: 0.0997\n",
      "\n",
      "Test accuracy: 0.09966666996479034\n",
      "219/219 [==============================] - 40s 184ms/step - loss: 2.3035 - sparse_categorical_accuracy: 0.0971\n",
      "94/94 [==============================] - 2s 26ms/step - loss: 2.3035 - sparse_categorical_accuracy: 0.0957\n",
      "\n",
      "Test accuracy: 0.09566666930913925\n",
      "219/219 [==============================] - 42s 193ms/step - loss: 2.3027 - sparse_categorical_accuracy: 0.1019\n",
      "94/94 [==============================] - 3s 37ms/step - loss: 2.3044 - sparse_categorical_accuracy: 0.0960\n",
      "\n",
      "Test accuracy: 0.09600000083446503\n",
      "219/219 [==============================] - 53s 240ms/step - loss: 2.3032 - sparse_categorical_accuracy: 0.0961\n",
      "94/94 [==============================] - 3s 30ms/step - loss: 2.3039 - sparse_categorical_accuracy: 0.0960\n",
      "\n",
      "Test accuracy: 0.09600000083446503\n",
      "219/219 [==============================] - 49s 223ms/step - loss: 2.3065 - sparse_categorical_accuracy: 0.0964\n",
      "94/94 [==============================] - 3s 32ms/step - loss: 2.3053 - sparse_categorical_accuracy: 0.0957\n",
      "\n",
      "Test accuracy: 0.09566666930913925\n",
      "219/219 [==============================] - 48s 220ms/step - loss: 2.3036 - sparse_categorical_accuracy: 0.0967\n",
      "94/94 [==============================] - 3s 32ms/step - loss: 2.3060 - sparse_categorical_accuracy: 0.0970\n",
      "\n",
      "Test accuracy: 0.09700000286102295\n",
      "219/219 [==============================] - 44s 203ms/step - loss: 2.3043 - sparse_categorical_accuracy: 0.0954\n",
      "94/94 [==============================] - 3s 31ms/step - loss: 2.3158 - sparse_categorical_accuracy: 0.0970\n",
      "\n",
      "Test accuracy: 0.09700000286102295\n",
      "219/219 [==============================] - 43s 199ms/step - loss: 2.3038 - sparse_categorical_accuracy: 0.0970\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 2.3093 - sparse_categorical_accuracy: 0.0960\n",
      "\n",
      "Test accuracy: 0.09600000083446503\n",
      "219/219 [==============================] - 43s 197ms/step - loss: 2.3040 - sparse_categorical_accuracy: 0.0986\n",
      "94/94 [==============================] - 3s 32ms/step - loss: 2.3079 - sparse_categorical_accuracy: 0.0957\n",
      "\n",
      "Test accuracy: 0.09566666930913925\n",
      "219/219 [==============================] - 43s 198ms/step - loss: 2.3015 - sparse_categorical_accuracy: 0.0951\n",
      "94/94 [==============================] - 3s 30ms/step - loss: 2.4302 - sparse_categorical_accuracy: 0.0957\n",
      "\n",
      "Test accuracy: 0.09566666930913925\n",
      "219/219 [==============================] - 43s 197ms/step - loss: 2.3012 - sparse_categorical_accuracy: 0.1001\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 2.3662 - sparse_categorical_accuracy: 0.0957\n",
      "\n",
      "Test accuracy: 0.09566666930913925\n",
      "219/219 [==============================] - 44s 200ms/step - loss: 2.3009 - sparse_categorical_accuracy: 0.0947\n",
      "94/94 [==============================] - 3s 32ms/step - loss: 2.3451 - sparse_categorical_accuracy: 0.0957\n",
      "\n",
      "Test accuracy: 0.09566666930913925\n",
      "219/219 [==============================] - 45s 203ms/step - loss: 2.3006 - sparse_categorical_accuracy: 0.0987\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 2.3903 - sparse_categorical_accuracy: 0.0957\n",
      "\n",
      "Test accuracy: 0.09566666930913925\n",
      "219/219 [==============================] - 44s 200ms/step - loss: 2.3007 - sparse_categorical_accuracy: 0.0939\n",
      "94/94 [==============================] - 3s 31ms/step - loss: 2.3924 - sparse_categorical_accuracy: 0.0957\n",
      "\n",
      "Test accuracy: 0.09566666930913925\n",
      "219/219 [==============================] - 43s 198ms/step - loss: 2.3007 - sparse_categorical_accuracy: 0.0990\n",
      "94/94 [==============================] - 3s 33ms/step - loss: 2.3429 - sparse_categorical_accuracy: 0.0957\n",
      "\n",
      "Test accuracy: 0.09566666930913925\n",
      "219/219 [==============================] - 43s 197ms/step - loss: 2.3010 - sparse_categorical_accuracy: 0.0961\n",
      "94/94 [==============================] - 3s 31ms/step - loss: 2.4243 - sparse_categorical_accuracy: 0.0957\n",
      "\n",
      "Test accuracy: 0.09566666930913925\n",
      "219/219 [==============================] - 43s 197ms/step - loss: 2.3011 - sparse_categorical_accuracy: 0.0996\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 2.4742 - sparse_categorical_accuracy: 0.0957\n",
      "\n",
      "Test accuracy: 0.09566666930913925\n",
      "219/219 [==============================] - 44s 200ms/step - loss: 2.3017 - sparse_categorical_accuracy: 0.0964\n",
      "94/94 [==============================] - 3s 30ms/step - loss: 2.4618 - sparse_categorical_accuracy: 0.0953\n",
      "\n",
      "Test accuracy: 0.09533333033323288\n",
      "219/219 [==============================] - 44s 199ms/step - loss: 2.3008 - sparse_categorical_accuracy: 0.0997\n",
      "94/94 [==============================] - 3s 30ms/step - loss: 2.3962 - sparse_categorical_accuracy: 0.0960\n",
      "\n",
      "Test accuracy: 0.09600000083446503\n",
      "219/219 [==============================] - 43s 198ms/step - loss: 2.2990 - sparse_categorical_accuracy: 0.1014\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 2.4046 - sparse_categorical_accuracy: 0.0963\n",
      "\n",
      "Test accuracy: 0.0963333323597908\n",
      "219/219 [==============================] - 48s 220ms/step - loss: 2.3025 - sparse_categorical_accuracy: 0.1001\n",
      "94/94 [==============================] - 4s 42ms/step - loss: 2.3990 - sparse_categorical_accuracy: 0.0950\n",
      "\n",
      "Test accuracy: 0.0949999988079071\n",
      "219/219 [==============================] - 50s 227ms/step - loss: 2.2991 - sparse_categorical_accuracy: 0.0954\n",
      "94/94 [==============================] - 3s 31ms/step - loss: 2.3925 - sparse_categorical_accuracy: 0.0960\n",
      "\n",
      "Test accuracy: 0.09600000083446503\n",
      "219/219 [==============================] - 47s 215ms/step - loss: 2.2988 - sparse_categorical_accuracy: 0.0996\n",
      "94/94 [==============================] - 3s 33ms/step - loss: 2.4081 - sparse_categorical_accuracy: 0.0967\n",
      "\n",
      "Test accuracy: 0.09666666388511658\n",
      "219/219 [==============================] - 46s 211ms/step - loss: 2.2988 - sparse_categorical_accuracy: 0.1011\n",
      "94/94 [==============================] - 3s 33ms/step - loss: 2.4101 - sparse_categorical_accuracy: 0.0960\n",
      "\n",
      "Test accuracy: 0.09600000083446503\n",
      "219/219 [==============================] - 48s 221ms/step - loss: 2.2981 - sparse_categorical_accuracy: 0.1001\n",
      "94/94 [==============================] - 3s 32ms/step - loss: 2.4350 - sparse_categorical_accuracy: 0.0967\n",
      "\n",
      "Test accuracy: 0.09666666388511658\n",
      "219/219 [==============================] - 47s 216ms/step - loss: 2.2979 - sparse_categorical_accuracy: 0.1021\n",
      "94/94 [==============================] - 3s 31ms/step - loss: 2.4554 - sparse_categorical_accuracy: 0.0967\n",
      "\n",
      "Test accuracy: 0.09666666388511658\n",
      "219/219 [==============================] - 51s 234ms/step - loss: 2.2980 - sparse_categorical_accuracy: 0.0994\n",
      "94/94 [==============================] - 3s 31ms/step - loss: 2.4571 - sparse_categorical_accuracy: 0.0963\n",
      "\n",
      "Test accuracy: 0.0963333323597908\n",
      "219/219 [==============================] - 50s 228ms/step - loss: 2.2980 - sparse_categorical_accuracy: 0.1020\n",
      "94/94 [==============================] - 3s 35ms/step - loss: 2.4586 - sparse_categorical_accuracy: 0.0963\n",
      "\n",
      "Test accuracy: 0.0963333323597908\n",
      "219/219 [==============================] - 48s 220ms/step - loss: 2.2980 - sparse_categorical_accuracy: 0.0993\n",
      "94/94 [==============================] - 3s 32ms/step - loss: 2.4601 - sparse_categorical_accuracy: 0.0963\n",
      "\n",
      "Test accuracy: 0.0963333323597908\n",
      "219/219 [==============================] - 45s 206ms/step - loss: 2.2980 - sparse_categorical_accuracy: 0.1011\n",
      "94/94 [==============================] - 3s 33ms/step - loss: 2.4616 - sparse_categorical_accuracy: 0.0970\n",
      "\n",
      "Test accuracy: 0.09700000286102295\n",
      "219/219 [==============================] - 46s 212ms/step - loss: 2.2981 - sparse_categorical_accuracy: 0.0993\n",
      "94/94 [==============================] - 3s 37ms/step - loss: 2.4630 - sparse_categorical_accuracy: 0.0963\n",
      "\n",
      "Test accuracy: 0.0963333323597908\n",
      "219/219 [==============================] - 50s 228ms/step - loss: 2.2979 - sparse_categorical_accuracy: 0.1009\n",
      "94/94 [==============================] - 3s 35ms/step - loss: 2.4641 - sparse_categorical_accuracy: 0.0963\n",
      "\n",
      "Test accuracy: 0.0963333323597908\n",
      "219/219 [==============================] - 48s 220ms/step - loss: 2.2980 - sparse_categorical_accuracy: 0.1011\n",
      "94/94 [==============================] - 4s 37ms/step - loss: 2.4657 - sparse_categorical_accuracy: 0.0963\n",
      "\n",
      "Test accuracy: 0.0963333323597908\n",
      "219/219 [==============================] - 51s 231ms/step - loss: 2.2980 - sparse_categorical_accuracy: 0.1039\n",
      "94/94 [==============================] - 3s 33ms/step - loss: 2.4669 - sparse_categorical_accuracy: 0.0963\n",
      "\n",
      "Test accuracy: 0.0963333323597908\n",
      "219/219 [==============================] - 48s 218ms/step - loss: 2.2980 - sparse_categorical_accuracy: 0.0979\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 2.4682 - sparse_categorical_accuracy: 0.0963\n",
      "\n",
      "Test accuracy: 0.0963333323597908\n",
      "219/219 [==============================] - 48s 217ms/step - loss: 2.2980 - sparse_categorical_accuracy: 0.1000\n",
      "94/94 [==============================] - 3s 33ms/step - loss: 2.4694 - sparse_categorical_accuracy: 0.0963\n",
      "\n",
      "Test accuracy: 0.0963333323597908\n",
      "219/219 [==============================] - 49s 222ms/step - loss: 2.2980 - sparse_categorical_accuracy: 0.0989\n",
      "94/94 [==============================] - 3s 30ms/step - loss: 2.4706 - sparse_categorical_accuracy: 0.0980\n",
      "\n",
      "Test accuracy: 0.09799999743700027\n",
      "219/219 [==============================] - 50s 228ms/step - loss: 2.2980 - sparse_categorical_accuracy: 0.1004\n",
      "94/94 [==============================] - 3s 32ms/step - loss: 2.4715 - sparse_categorical_accuracy: 0.0963\n",
      "\n",
      "Test accuracy: 0.0963333323597908\n",
      "219/219 [==============================] - 48s 220ms/step - loss: 2.2980 - sparse_categorical_accuracy: 0.0969\n",
      "94/94 [==============================] - 3s 34ms/step - loss: 2.4728 - sparse_categorical_accuracy: 0.0963\n",
      "\n",
      "Test accuracy: 0.0963333323597908\n",
      "219/219 [==============================] - 48s 221ms/step - loss: 2.2980 - sparse_categorical_accuracy: 0.0973\n",
      "94/94 [==============================] - 3s 33ms/step - loss: 2.4738 - sparse_categorical_accuracy: 0.0970\n",
      "\n",
      "Test accuracy: 0.09700000286102295\n",
      "219/219 [==============================] - 50s 227ms/step - loss: 2.2979 - sparse_categorical_accuracy: 0.0984\n",
      "94/94 [==============================] - 3s 36ms/step - loss: 2.4749 - sparse_categorical_accuracy: 0.0963\n",
      "\n",
      "Test accuracy: 0.0963333323597908\n",
      "219/219 [==============================] - 49s 223ms/step - loss: 2.2979 - sparse_categorical_accuracy: 0.0996\n",
      "94/94 [==============================] - 3s 31ms/step - loss: 2.4752 - sparse_categorical_accuracy: 0.0967\n",
      "\n",
      "Test accuracy: 0.09666666388511658\n",
      "219/219 [==============================] - 47s 214ms/step - loss: 2.2980 - sparse_categorical_accuracy: 0.0970\n",
      "94/94 [==============================] - 3s 33ms/step - loss: 2.4773 - sparse_categorical_accuracy: 0.0963\n",
      "\n",
      "Test accuracy: 0.0963333323597908\n",
      "219/219 [==============================] - 48s 219ms/step - loss: 2.2979 - sparse_categorical_accuracy: 0.0981\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 2.4804 - sparse_categorical_accuracy: 0.0963\n",
      "\n",
      "Test accuracy: 0.0963333323597908\n",
      "219/219 [==============================] - 46s 211ms/step - loss: 2.2980 - sparse_categorical_accuracy: 0.1027\n",
      "94/94 [==============================] - 3s 32ms/step - loss: 2.4814 - sparse_categorical_accuracy: 0.0963\n",
      "\n",
      "Test accuracy: 0.0963333323597908\n",
      "219/219 [==============================] - 45s 206ms/step - loss: 2.2980 - sparse_categorical_accuracy: 0.0989\n",
      "94/94 [==============================] - 3s 30ms/step - loss: 2.4853 - sparse_categorical_accuracy: 0.0963\n",
      "\n",
      "Test accuracy: 0.0963333323597908\n",
      "219/219 [==============================] - 45s 204ms/step - loss: 2.2979 - sparse_categorical_accuracy: 0.1014\n",
      "94/94 [==============================] - 3s 29ms/step - loss: 2.4862 - sparse_categorical_accuracy: 0.0967\n",
      "\n",
      "Test accuracy: 0.09666666388511658\n",
      "219/219 [==============================] - 45s 206ms/step - loss: 2.2979 - sparse_categorical_accuracy: 0.0976\n",
      "94/94 [==============================] - 3s 32ms/step - loss: 2.4791 - sparse_categorical_accuracy: 0.0963\n",
      "\n",
      "Test accuracy: 0.0963333323597908\n",
      "219/219 [==============================] - 46s 208ms/step - loss: 2.3106 - sparse_categorical_accuracy: 0.0996\n",
      "94/94 [==============================] - 3s 31ms/step - loss: 2.4489 - sparse_categorical_accuracy: 0.0987\n",
      "\n",
      "Test accuracy: 0.09866666793823242\n",
      "219/219 [==============================] - 47s 215ms/step - loss: 2.2986 - sparse_categorical_accuracy: 0.0974\n",
      "94/94 [==============================] - 4s 40ms/step - loss: 2.4197 - sparse_categorical_accuracy: 0.0977\n",
      "\n",
      "Test accuracy: 0.0976666659116745\n",
      "219/219 [==============================] - 47s 216ms/step - loss: 2.2985 - sparse_categorical_accuracy: 0.0983\n",
      "94/94 [==============================] - 3s 30ms/step - loss: 2.4584 - sparse_categorical_accuracy: 0.0970\n",
      "\n",
      "Test accuracy: 0.09700000286102295\n",
      "219/219 [==============================] - 48s 218ms/step - loss: 2.2983 - sparse_categorical_accuracy: 0.0996\n",
      "94/94 [==============================] - 3s 32ms/step - loss: 2.4628 - sparse_categorical_accuracy: 0.0963\n",
      "\n",
      "Test accuracy: 0.0963333323597908\n",
      "219/219 [==============================] - 49s 222ms/step - loss: 2.2983 - sparse_categorical_accuracy: 0.0953\n",
      "94/94 [==============================] - 3s 33ms/step - loss: 2.4628 - sparse_categorical_accuracy: 0.0963\n",
      "\n",
      "Test accuracy: 0.0963333323597908\n",
      "219/219 [==============================] - 46s 208ms/step - loss: 2.2983 - sparse_categorical_accuracy: 0.1037\n",
      "94/94 [==============================] - 3s 31ms/step - loss: 2.4629 - sparse_categorical_accuracy: 0.0963\n",
      "\n",
      "Test accuracy: 0.0963333323597908\n",
      "219/219 [==============================] - 50s 226ms/step - loss: 2.2982 - sparse_categorical_accuracy: 0.1037\n",
      "94/94 [==============================] - 3s 34ms/step - loss: 2.4637 - sparse_categorical_accuracy: 0.0963\n",
      "\n",
      "Test accuracy: 0.0963333323597908\n",
      "219/219 [==============================] - 48s 220ms/step - loss: 2.2982 - sparse_categorical_accuracy: 0.0989\n",
      "94/94 [==============================] - 3s 32ms/step - loss: 2.4638 - sparse_categorical_accuracy: 0.0970\n",
      "\n",
      "Test accuracy: 0.09700000286102295\n",
      "219/219 [==============================] - 49s 225ms/step - loss: 2.2983 - sparse_categorical_accuracy: 0.1004\n",
      "94/94 [==============================] - 3s 31ms/step - loss: 2.4639 - sparse_categorical_accuracy: 0.0963\n",
      "\n",
      "Test accuracy: 0.0963333323597908\n",
      "219/219 [==============================] - 50s 228ms/step - loss: 2.2983 - sparse_categorical_accuracy: 0.0981\n",
      "94/94 [==============================] - 3s 32ms/step - loss: 2.4640 - sparse_categorical_accuracy: 0.0970\n",
      "\n",
      "Test accuracy: 0.09700000286102295\n",
      "219/219 [==============================] - 47s 213ms/step - loss: 2.2983 - sparse_categorical_accuracy: 0.1024\n",
      "94/94 [==============================] - 4s 37ms/step - loss: 2.4652 - sparse_categorical_accuracy: 0.0970\n",
      "\n",
      "Test accuracy: 0.09700000286102295\n",
      "219/219 [==============================] - 49s 223ms/step - loss: 2.2983 - sparse_categorical_accuracy: 0.0999\n",
      "94/94 [==============================] - 4s 39ms/step - loss: 2.4654 - sparse_categorical_accuracy: 0.0977\n",
      "\n",
      "Test accuracy: 0.0976666659116745\n",
      "219/219 [==============================] - 45s 205ms/step - loss: 2.2983 - sparse_categorical_accuracy: 0.0934\n",
      "94/94 [==============================] - 3s 31ms/step - loss: 2.4656 - sparse_categorical_accuracy: 0.0970\n",
      "\n",
      "Test accuracy: 0.09700000286102295\n",
      "219/219 [==============================] - 51s 235ms/step - loss: 2.2983 - sparse_categorical_accuracy: 0.0986\n",
      "94/94 [==============================] - 5s 51ms/step - loss: 2.4658 - sparse_categorical_accuracy: 0.0963\n",
      "\n",
      "Test accuracy: 0.0963333323597908\n",
      "116/219 [==============>...............] - ETA: 34s - loss: 2.2975 - sparse_categorical_accuracy: 0.0935 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[131], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mFFNN_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tr_nn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_tr_nn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m     test_loss, test_acc \u001b[38;5;241m=\u001b[39m FFNN_model\u001b[38;5;241m.\u001b[39mevaluate(X_te_nn, y_te_nn)\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTest accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_acc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\cs345CondaEnv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\.conda\\envs\\cs345CondaEnv\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\.conda\\envs\\cs345CondaEnv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\.conda\\envs\\cs345CondaEnv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\.conda\\envs\\cs345CondaEnv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\.conda\\envs\\cs345CondaEnv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\cs345CondaEnv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\.conda\\envs\\cs345CondaEnv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\.conda\\envs\\cs345CondaEnv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(100):\n",
    "    FFNN_model.fit(X_tr_nn, y_tr_nn)\n",
    "    test_loss, test_acc = FFNN_model.evaluate(X_te_nn, y_te_nn)\n",
    "    print(f'\\nTest accuracy: {test_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cff1bfc-65a2-49df-9685-7a18445a2b31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
