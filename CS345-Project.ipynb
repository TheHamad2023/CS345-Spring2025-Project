{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4315aa3e",
   "metadata": {},
   "source": [
    "# CS345 Project\n",
    "\n",
    "## Team Members\n",
    "1. Hamad Alyami\n",
    "2. Benito Encarnacion\n",
    "\n",
    "## Dataset\n",
    "Our dataset was from Kaggle by a user called Mexwell. The data is paragraphs scraped from wikipedia in 2018 in 235 languages.\n",
    "\n",
    "The dataset contains 235,000 datasets with balance between language proportions and a test and train split provided.\n",
    "\n",
    "The downloaded folder from Kaggle contains:\n",
    "- labels.csv: A file containing the language name, 2-3 letter code, German name, and language family of all the languages present in the dataset.\n",
    "- README.txt: A file explaining the folder contents.\n",
    "- urls.txt: A file containing the urls of where the paragraphs were found.\n",
    "- x_test.txt: The testing data samples, paragraphs in multiple languages.\n",
    "- x_train.txt: The training data samples, paragraphs in multiple langauges\n",
    "- y_test.txt: The labels for the testing dataset, using the 2-3 letter codes found in labels.csv.\n",
    "- y_train.txt: The labels for the training dataset, using the 2-3 letter codes found in labels.csv\n",
    "\n",
    "\n",
    "## Project\n",
    "Our project is to train and compare two ML models on the Latin Alphabet languages present in the dataset and compare their performance.\n",
    "\n",
    "## Motivation\n",
    "We decided to do this project because it allows us to explore practical applications of natural language processing and machine learning by working with real-world multilingual data. Language identification is an important task in many systems and applications like search engines, translation tools, and content moderation. Working with such a dataset gives us the opportunity to apply classification techniques in a meaningful way. By focusing on languages that use the Latin alphabet, we avoid complications from different writing systems while still working with a variety of languages.\n",
    "\n",
    "## Models\n",
    "The models we decided to work with in this project are:\n",
    "- Multinomial Naive-Bayes (MNB): Uses word frequencies in each class, langauges in our case, to guess the most likely class for text it has not seen.\n",
    "\n",
    "- Feed Forward Neural Network (FNN): An artificial Neural Network where information moves from input to output without looping back. It uses neurons, connected nodes, to learn patterns and make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1704ce66",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "We will begin by reading the data from the files then:\n",
    "1. Remove Null Values\n",
    "2. Filter to keep texts of languages we want using the 2-3 letter codes\n",
    "3. Return both samples from x_test and x_train and labels from y_test and y_train stacked into X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "7602a789-a084-4f24-b977-47df8039c2a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data/x_train.txt: Read!\n",
      "Data/y_train.txt: Read!\n",
      "Data/x_test.txt: Read!\n",
      "Data/y_test.txt: Read!\n",
      "(10000,) (10000,)\n"
     ]
    }
   ],
   "source": [
    "#Understanding the data set\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "            #Italina, French, Spanish, Portugese, English, German, Dutch, Indonesian, Finnish, Hausa\n",
    "lang_codes = ['ita', 'fra', 'spa', 'por', 'eng', 'deu', 'nld', 'ind', 'fin', 'hau']\n",
    "langs = ['Italian', 'French', 'Spanish', 'Portuguese', 'English', 'German', 'Dutch', 'Indonesian', 'Finnish', 'Hausa']\n",
    "\n",
    "def file_to_np_array(path, label):\n",
    "    try:\n",
    "        df = pd.read_csv(path, sep='<NonExistenceSeparator>', header=None, engine='python')\n",
    "        print(f\"{label}: Read!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading the {label} file: {e}\")\n",
    "        return None\n",
    "    return df.to_numpy()\n",
    "\n",
    "\n",
    "def clean_np_data(X, y):\n",
    "    stacked = np.hstack((y, X)) # Stack y and X side by side\n",
    "    # print(stacked.shape)\n",
    "    clean_stacked = stacked[~np.any(pd.isna(stacked), axis=1), :] # Remove empty values\n",
    "    # print(clean_stacked.shape)\n",
    "    true_clean = clean_stacked[np.isin(clean_stacked[:,0], lang_codes),:] # Remove all rows that aren't our target languages\n",
    "    # print(true_clean.shape)\n",
    "    return true_clean[:,1], true_clean[:,0] # Return cleaned as X and y split again\n",
    "\n",
    "def clean_filter_and_stack(X_train_file, y_train_file, X_test_file, y_test_file):\n",
    "    X_train_clean, y_train_clean = clean_np_data(file_to_np_array(X_train_file, X_train_file), \n",
    "                                       file_to_np_array(y_train_file, y_train_file))\n",
    "    X_test_clean, y_test_clean = clean_np_data(file_to_np_array(X_test_file, X_test_file), \n",
    "                                       file_to_np_array(y_test_file, y_test_file))\n",
    "    return np.hstack((X_train_clean, X_test_clean)), np.hstack((y_train_clean, y_test_clean))\n",
    "\n",
    "X, y = clean_filter_and_stack(\"Data/x_train.txt\", \n",
    "                                      \"Data/y_train.txt\", \n",
    "                                      \"Data/x_test.txt\", \n",
    "                                      \"Data/y_test.txt\")\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b5f87e",
   "metadata": {},
   "source": [
    "#### Data Discovery\n",
    "This code is to find what is the sample distribution between languages and average word count of each sample of each language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "d27cb38f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Language</th>\n",
       "      <th>Percent of Dataset (%)</th>\n",
       "      <th>Average Word Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Italian</td>\n",
       "      <td>10.0</td>\n",
       "      <td>68.192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>French</td>\n",
       "      <td>10.0</td>\n",
       "      <td>67.707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Spanish</td>\n",
       "      <td>10.0</td>\n",
       "      <td>67.295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Portuguese</td>\n",
       "      <td>10.0</td>\n",
       "      <td>66.184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>English</td>\n",
       "      <td>10.0</td>\n",
       "      <td>70.455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>German</td>\n",
       "      <td>10.0</td>\n",
       "      <td>59.762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Dutch</td>\n",
       "      <td>10.0</td>\n",
       "      <td>55.657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Indonesian</td>\n",
       "      <td>10.0</td>\n",
       "      <td>57.147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Finnish</td>\n",
       "      <td>10.0</td>\n",
       "      <td>48.431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Hausa</td>\n",
       "      <td>10.0</td>\n",
       "      <td>75.802</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Language  Percent of Dataset (%)  Average Word Count\n",
       "0     Italian                    10.0              68.192\n",
       "1      French                    10.0              67.707\n",
       "2     Spanish                    10.0              67.295\n",
       "3  Portuguese                    10.0              66.184\n",
       "4     English                    10.0              70.455\n",
       "5      German                    10.0              59.762\n",
       "6       Dutch                    10.0              55.657\n",
       "7  Indonesian                    10.0              57.147\n",
       "8     Finnish                    10.0              48.431\n",
       "9       Hausa                    10.0              75.802"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def avg_words(filtered_X):\n",
    "    total = 0\n",
    "    for text in filtered_X:\n",
    "        words = str(text).split()\n",
    "        total += len(words)\n",
    "\n",
    "    return total / len(filtered_X)\n",
    "\n",
    "def word_count_perlang(X, y):\n",
    "    avg_word_count = []\n",
    "    for lang in lang_codes:\n",
    "        filtered_X = X[y == lang]\n",
    "        avg_word_count.append(avg_words(filtered_X))\n",
    "    \n",
    "    return avg_word_count\n",
    "\n",
    "def lang_perc(y):\n",
    "    lang_perc = []\n",
    "    total = len(y)\n",
    "    for lang in lang_codes:\n",
    "        count = (y == lang).sum()\n",
    "        percent = (count / total) * 100\n",
    "        lang_perc.append(percent)\n",
    "    return lang_perc\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Language': langs,\n",
    "    'Percent of Dataset (%)': lang_perc(y),\n",
    "    'Average Word Count': word_count_perlang(X, y)\n",
    "})\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a7d528",
   "metadata": {},
   "source": [
    "#### Data Split\n",
    "Here we use Sklearn train_test_split to split our data into 70/30 train and test splits, respectively, after shuffling them randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "60b758fc-1a04-4cc4-8c94-9c6034e4ca00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7000,) (7000,)\n",
      "(3000,) (3000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=17)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad363514",
   "metadata": {},
   "source": [
    "And then vectorize our dataset for the MNB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "a5925257-f51e-452b-be5b-7a4dede72735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done vectorizing\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(strip_accents='unicode')\n",
    "X_train_vectors = vectorizer.fit_transform(X_train)\n",
    "X_test_vectors = vectorizer.transform(X_test)\n",
    "print(\"Done vectorizing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7602be5a",
   "metadata": {},
   "source": [
    "We train our MNB using the MultinomialNB() function from sklearn on X_train which is 70% of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb703257-141d-403f-a819-31553f0ccde4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training MNB\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train_vectors, y_train)\n",
    "print(\"Done training MNB\")\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = model.predict(X_test_vectors)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Overall accuracy of MNB: \" + str(accuracy * 100) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e9320b-e9e6-4217-8dc6-e068913a34f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy of MNB: 98.1%\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "d6603099",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Language</th>\n",
       "      <th>Accuracy of MNB/language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Italian</td>\n",
       "      <td>96.855346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>French</td>\n",
       "      <td>99.335548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Spanish</td>\n",
       "      <td>97.647059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Portuguese</td>\n",
       "      <td>93.728223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>English</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>German</td>\n",
       "      <td>97.569444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Dutch</td>\n",
       "      <td>98.275862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Indonesian</td>\n",
       "      <td>98.615917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Finnish</td>\n",
       "      <td>99.315068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Hausa</td>\n",
       "      <td>99.662162</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Language  Accuracy of MNB/language\n",
       "0     Italian                 96.855346\n",
       "1      French                 99.335548\n",
       "2     Spanish                 97.647059\n",
       "3  Portuguese                 93.728223\n",
       "4     English                100.000000\n",
       "5      German                 97.569444\n",
       "6       Dutch                 98.275862\n",
       "7  Indonesian                 98.615917\n",
       "8     Finnish                 99.315068\n",
       "9       Hausa                 99.662162"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_lang_accuracies(y_true, y_pred):\n",
    "    df = pd.DataFrame({'language': y_true, 'pred': y_pred})\n",
    "    accuracies = []\n",
    "\n",
    "    for lang in lang_codes:\n",
    "        lang_group = df[df['language'] == lang]\n",
    "        if len(lang_group) > 0:\n",
    "            acc = accuracy_score(lang_group['language'], lang_group['pred'])\n",
    "        else:\n",
    "            acc = 0\n",
    "        accuracies.append(acc)\n",
    "    \n",
    "    percent_accuracies = [x * 100 for x in accuracies]\n",
    "    df = None\n",
    "    return percent_accuracies\n",
    "\n",
    "df_MNB_lang = pd.DataFrame({\n",
    "    'Language': langs,\n",
    "    'Accuracy of MNB/language': get_lang_accuracies(y_test, y_pred)\n",
    "})\n",
    "\n",
    "display(df_MNB_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "b894d5c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Pipeline' object has no attribute 'feature_log_prob_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[140], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m classes \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mclasses_\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Get the log probabilities of features per class\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m log_probs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfeature_log_prob_  \u001b[38;5;66;03m# shape: [n_classes, n_features]\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Get the vocabulary mapping from the vectorizer\u001b[39;00m\n\u001b[1;32m      8\u001b[0m feature_names \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mget_feature_names_out()  \u001b[38;5;66;03m# assumes CountVectorizer or TfidfVectorizer\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Pipeline' object has no attribute 'feature_log_prob_'"
     ]
    }
   ],
   "source": [
    "# Get the class labels (languages)\n",
    "classes = model.classes_\n",
    "\n",
    "# Get the log probabilities of features per class\n",
    "log_probs = model.feature_log_prob_  # shape: [n_classes, n_features]\n",
    "\n",
    "# Get the vocabulary mapping from the vectorizer\n",
    "feature_names = vectorizer.get_feature_names_out()  # assumes CountVectorizer or TfidfVectorizer\n",
    "\n",
    "for idx, lang in enumerate(classes):\n",
    "    top_word_idx = log_probs[idx].argmax()  # index of the most likely word for this class\n",
    "    top_word = feature_names[top_word_idx]\n",
    "    print(f\"Most indicative word for {lang}: {top_word}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "2f966d3e-57e4-4b15-9897-32ca8d142f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "model = make_pipeline(CountVectorizer(), MultinomialNB())\n",
    "num_folds = 5\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=17)\n",
    "\n",
    "all_preds = []\n",
    "all_true = []\n",
    "\n",
    "X_train_kfold = X_train.copy()\n",
    "X_test_kfold = X_test.copy()\n",
    "y_train_kfold = y_train.copy()\n",
    "y_test_kfold = y_test.copy()\n",
    "\n",
    "k_folds_accuracies = []\n",
    "for i, (train_index, test_index) in enumerate(kf.split(X), 1):\n",
    "    X_train_kfold, X_test_kfold = X[train_index], X[test_index]\n",
    "    y_train_kfold, y_test_kfold = y[train_index], y[test_index]\n",
    "\n",
    "    model.fit(X_train_kfold.flatten(), y_train_kfold.flatten())\n",
    "    y_pred_kfold = model.predict(X_test_kfold.flatten())\n",
    "\n",
    "    fold_accuracy = accuracy_score(y_test_kfold, y_pred_kfold)\n",
    "    k_folds_accuracies.append(fold_accuracy)\n",
    "\n",
    "    all_preds.extend(y_pred_kfold)\n",
    "    all_true.extend(y_test_kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "c828e604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Results (Accuracy):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Kfold</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.9825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.9830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.9825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.9860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.9840</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Kfold  Accuracy\n",
       "0      1    0.9825\n",
       "1      2    0.9830\n",
       "2      3    0.9825\n",
       "3      4    0.9860\n",
       "4      5    0.9840"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Kfolds Mean Accuracy: 98.36%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Language</th>\n",
       "      <th>Accuracy of MNB/language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Italian</td>\n",
       "      <td>0.985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>French</td>\n",
       "      <td>0.993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Spanish</td>\n",
       "      <td>0.980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Portuguese</td>\n",
       "      <td>0.949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>English</td>\n",
       "      <td>0.998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>German</td>\n",
       "      <td>0.982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Dutch</td>\n",
       "      <td>0.980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Indonesian</td>\n",
       "      <td>0.978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Finnish</td>\n",
       "      <td>0.996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Hausa</td>\n",
       "      <td>0.995</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Language  Accuracy of MNB/language\n",
       "0     Italian                     0.985\n",
       "1      French                     0.993\n",
       "2     Spanish                     0.980\n",
       "3  Portuguese                     0.949\n",
       "4     English                     0.998\n",
       "5      German                     0.982\n",
       "6       Dutch                     0.980\n",
       "7  Indonesian                     0.978\n",
       "8     Finnish                     0.996\n",
       "9       Hausa                     0.995"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_kfold_accuracy = pd.DataFrame({\n",
    "    'Kfold': [1, 2, 3, 4, 5],\n",
    "    'Accuracy': k_folds_accuracies\n",
    "})\n",
    "\n",
    "\n",
    "print(\"Cross-Validation Results (Accuracy):\")\n",
    "\n",
    "display(df_kfold_accuracy)\n",
    "\n",
    "print(f'\\nOverall Kfolds Mean Accuracy: {cross_val_score(model, X.flatten(), y.flatten(), cv=kf).mean() * 100:}%')\n",
    "\n",
    "def get_lang_accuracies(y_true, y_pred):\n",
    "    df = pd.DataFrame({'language': y_true, 'pred': y_pred})\n",
    "    accuracies = []\n",
    "\n",
    "    for lang in lang_codes:\n",
    "        lang_group = df[df['language'] == lang]\n",
    "        if len(lang_group) > 0:\n",
    "            acc = accuracy_score(lang_group['language'], lang_group['pred'])\n",
    "        else:\n",
    "            acc = 0\n",
    "        accuracies.append(acc)\n",
    "\n",
    "    return accuracies\n",
    "\n",
    "df_lang_accuracy = pd.DataFrame({\n",
    "    'Language': langs,\n",
    "    'Accuracy of MNB/language': get_lang_accuracies(np.array(all_true), np.array(all_preds))\n",
    "})\n",
    "\n",
    "display(df_lang_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "81a93297-6dd5-4cb8-af86-0ff9a0616fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorganizing data for Feed Forward Neural Network input\n",
    "\n",
    "# 1) convert string labels into integer labels\n",
    "#      and make func to convert back\n",
    "\n",
    "\n",
    "def str_labels_to_int_labels(labelArr, string_labels):\n",
    "    rtn = np.empty(labelArr.shape, dtype=int)\n",
    "    for i, v in enumerate(string_labels):\n",
    "        rtn[labelArr == v] = i\n",
    "    return rtn\n",
    "\n",
    "def int_labels_to_str_labels(labelArr, string_labels):\n",
    "    rtn = np.empty(labelArr.shape, dtype='object')\n",
    "    for i, v in enumerate(string_labels):\n",
    "        rtn[labelArr == i] = v\n",
    "    return rtn\n",
    "\n",
    "# print(y_test[0:5])\n",
    "# y1 = str_labels_to_int_labels(y_test, all_str_labels)\n",
    "# print(y1[0:5])\n",
    "# y2 = int_labels_to_str_labels(y1, all_str_labels)\n",
    "# print(y2[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "1be68002-eb66-461c-94b4-7fa37cc9e4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) convert data into multi-column matrix of characters\n",
    "#      and make func to convert back\n",
    "\n",
    "def str_vec_to_float_matrix(strVec, longest_str_len):\n",
    "    # Pad strings to all be equal length\n",
    "    padded_strVec = np.char.ljust(strVec, longest_str_len, fillchar=' ')\n",
    "\n",
    "    # turn vector of strings into matrix of characters\n",
    "    stacked_char_matrix = np.vstack([np.array(list(s)) for s in padded_strVec])\n",
    "\n",
    "    # turn char matrix into int matrix\n",
    "    char_matrix_to_int_matrix = np.vectorize(ord)\n",
    "    int_matrix = char_matrix_to_int_matrix(stacked_char_matrix)\n",
    "\n",
    "    #normalize and scale so each value is a float between 0 and 1\n",
    "    matrix_max = np.max(int_matrix)\n",
    "    matrix_min = np.min(int_matrix)\n",
    "    min_subtracted_matrix = int_matrix - matrix_min\n",
    "    normalized_matrix = (min_subtracted_matrix / (matrix_max - matrix_min))\n",
    "    return normalized_matrix\n",
    "\n",
    "# Don't need to convert matrices back into rows of text because the neural network isn't designed to generate anything, just classify\n",
    "# def int_matrix_to_str_vec(intMatrix):\n",
    "#     int_matrix_to_char_matrix = np.vectorize(chr)\n",
    "#     char_matrix = int_matrix_to_char_matrix(intMatrix)\n",
    "#     padded_strVec = np.array([\"\".join(r) for r in char_matrix])\n",
    "#     return np.char.rstrip(padded_strVec)\n",
    "\n",
    "# print(X_test[0])\n",
    "# X_x = str_vec_to_int_matrix(X_test.astype(str))\n",
    "# print(X_x[0])\n",
    "# X_y = int_matrix_to_str_vec(X_x)\n",
    "# print(X_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "58228e9e-a643-4b73-a51a-ba3366793109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done converting data into FFNN format\n"
     ]
    }
   ],
   "source": [
    "# 3) use them both\n",
    "\n",
    "def convert_to_FFNN_format(Xr, Xe, yr, ye):\n",
    "    all_string_labels = ['ita', 'fra', 'spa', 'por', 'eng', 'deu', 'nld', 'ind', 'fin', 'hau']\n",
    "    max_str_len_1 = np.max(np.char.str_len(Xr.astype(str)))\n",
    "    max_str_len_2 = np.max(np.char.str_len(Xe.astype(str)))\n",
    "    max_str_len = max(max_str_len_1, max_str_len_2)\n",
    "    Xr_rtn = str_vec_to_float_matrix(Xr.astype(str), max_str_len)\n",
    "    Xe_rtn = str_vec_to_float_matrix(Xe.astype(str), max_str_len)\n",
    "    return (Xr_rtn, \n",
    "            Xe_rtn,\n",
    "            str_labels_to_int_labels(yr, all_string_labels), \n",
    "            str_labels_to_int_labels(ye, all_string_labels))\n",
    "\n",
    "(X_tr_nn, X_te_nn, y_tr_nn, y_te_nn) = convert_to_FFNN_format(X_train, X_test, y_train, y_test)\n",
    "\n",
    "print(\"Done converting data into FFNN format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "c436646e-e772-42a2-9b08-ba6d866aaecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7000, 5577)\n",
      "(3000, 5577)\n"
     ]
    }
   ],
   "source": [
    "# print(X_tr_nn[0])\n",
    "# print(X_tr_nn[1])\n",
    "print(X_tr_nn.shape)\n",
    "print(X_te_nn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "07b30fd7-52e8-4fec-8803-69c7e421ef36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done importing Tensorflow Stuff\n"
     ]
    }
   ],
   "source": [
    "# Applying properly structured data to a basic FFNN\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import SparseCategoricalCrossentropy\n",
    "from keras.metrics import SparseCategoricalAccuracy\n",
    "\n",
    "print(\"Done importing Tensorflow Stuff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "f025e6da-c3bf-47bb-a146-ab488730faf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make FFNN\n",
    "\n",
    "sample_length = X_tr_nn[0].shape[0]\n",
    "\n",
    "FFNN_model = Sequential([\n",
    "    Dense(sample_length, activation='relu'),\n",
    "    Dense(sample_length, activation='relu'),\n",
    "    Dense(1024, activation='relu'),\n",
    "    Dense(1024, activation='relu'),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "FFNN_model.compile(optimizer=Adam(),\n",
    "                   loss=SparseCategoricalCrossentropy(), \n",
    "                   metrics=[SparseCategoricalAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "2a0f14b7-7541-4e8c-af62-cfb93a3dbe48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 39/219\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:12\u001b[0m 403ms/step - loss: 2.3036 - sparse_categorical_accuracy: 0.1074"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[137], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     FFNN_model\u001b[38;5;241m.\u001b[39mfit(X_tr_nn, y_tr_nn)\n\u001b[1;32m      3\u001b[0m     test_loss, test_acc \u001b[38;5;241m=\u001b[39m FFNN_model\u001b[38;5;241m.\u001b[39mevaluate(X_te_nn, y_te_nn)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTest accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_acc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/development/python/tools/anaconda3/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/development/python/tools/anaconda3/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:320\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[1;32m    319\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 320\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[1;32m    321\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[0;32m~/development/python/tools/anaconda3/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/development/python/tools/anaconda3/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/development/python/tools/anaconda3/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m tracing_compilation\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[1;32m    879\u001b[0m     args, kwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config\n\u001b[1;32m    880\u001b[0m )\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/development/python/tools/anaconda3/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function\u001b[38;5;241m.\u001b[39m_call_flat(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[38;5;241m=\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs\n\u001b[1;32m    141\u001b[0m )\n",
      "File \u001b[0;32m~/development/python/tools/anaconda3/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_function\u001b[38;5;241m.\u001b[39mcall_preflattened(args)\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/development/python/tools/anaconda3/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_flat(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/development/python/tools/anaconda3/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m    253\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    254\u001b[0m         \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mflat_outputs),\n\u001b[1;32m    255\u001b[0m     )\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/development/python/tools/anaconda3/lib/python3.12/site-packages/tensorflow/python/eager/context.py:1683\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1683\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[1;32m   1684\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1685\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   1686\u001b[0m       inputs\u001b[38;5;241m=\u001b[39mtensor_inputs,\n\u001b[1;32m   1687\u001b[0m       attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[1;32m   1688\u001b[0m       ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1689\u001b[0m   )\n\u001b[1;32m   1690\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1691\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1692\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1693\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1697\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1698\u001b[0m   )\n",
      "File \u001b[0;32m~/development/python/tools/anaconda3/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(1):\n",
    "    FFNN_model.fit(X_tr_nn, y_tr_nn)\n",
    "    test_loss, test_acc = FFNN_model.evaluate(X_te_nn, y_te_nn)\n",
    "    print(f'\\nTest accuracy: {test_acc}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
