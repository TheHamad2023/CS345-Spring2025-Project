{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4315aa3e",
   "metadata": {},
   "source": [
    "# CS345 Project\n",
    "\n",
    "## Team Members\n",
    "1. Hamad Alyami\n",
    "2. Benito Encarnacion\n",
    "\n",
    "## Dataset\n",
    "Our dataset was from Kaggle by a user called Mexwell. The data is paragraphs scraped from wikipedia in 2018 in 235 languages.\n",
    "\n",
    "The dataset contains 235,000 datasets with balance between language proportions and a test and train split provided.\n",
    "\n",
    "The downloaded folder from Kaggle contains:\n",
    "- labels.csv: A file containing the language name, 2-3 letter code, German name, and language family of all the languages present in the dataset.\n",
    "- README.txt: A file explaining the folder contents.\n",
    "- urls.txt: A file containing the urls of where the paragraphs were found.\n",
    "- x_test.txt: The testing data samples, paragraphs in multiple languages.\n",
    "- x_train.txt: The training data samples, paragraphs in multiple langauges\n",
    "- y_test.txt: The labels for the testing dataset, using the 2-3 letter codes found in labels.csv.\n",
    "- y_train.txt: The labels for the training dataset, using the 2-3 letter codes found in labels.csv\n",
    "\n",
    "\n",
    "## Project\n",
    "Our project is to train and compare two ML models on the Latin Alphabet languages present in the dataset and compare their performance.\n",
    "\n",
    "## Motivation\n",
    "We decided to do this project because it allows us to explore practical applications of natural language processing and machine learning by working with real-world multilingual data. Language identification is an important task in many systems and applications like search engines, translation tools, and content moderation. Working with such a dataset gives us the opportunity to apply classification techniques in a meaningful way. By focusing on languages that use the Latin alphabet, we avoid complications from different writing systems while still working with a variety of languages.\n",
    "\n",
    "## Models\n",
    "The models we decided to work with in this project are:\n",
    "- Multinomial Naive-Bayes (MNB): Uses word frequencies in each class, langauges in our case, to guess the most likely class for text it has not seen.\n",
    "\n",
    "- Feed Forward Neural Network (FNN): An artificial Neural Network where information moves from input to output without looping back. It uses neurons, connected nodes, to learn patterns and make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1704ce66",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "We will begin by reading the data from the files then:\n",
    "1. Remove Null Values\n",
    "2. Filter to keep texts of languages we want using the 2-3 letter codes\n",
    "3. Return both samples from x_test and x_train and labels from y_test and y_train stacked into X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7602a789-a084-4f24-b977-47df8039c2a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data/x_train.txt: Read!\n",
      "Data/y_train.txt: Read!\n",
      "Data/x_test.txt: Read!\n",
      "Data/y_test.txt: Read!\n",
      "(10000,) (10000,)\n"
     ]
    }
   ],
   "source": [
    "#Understanding the data set\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def file_to_np_array(path, label):\n",
    "    try:\n",
    "        df = pd.read_csv(path, sep='<NonExistenceSeparator>', header=None, engine='python')\n",
    "        print(f\"{label}: Read!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading the {label} file: {e}\")\n",
    "        return None\n",
    "    return df.to_numpy()\n",
    "\n",
    "\n",
    "def clean_np_data(X, y):\n",
    "    stacked = np.hstack((y, X)) # Stack y and X side by side\n",
    "    # print(stacked.shape)\n",
    "    clean_stacked = stacked[~np.any(pd.isna(stacked), axis=1), :] # Remove empty values\n",
    "    # print(clean_stacked.shape)\n",
    "    lang_codes = ['ita', 'fra', 'spa', 'por', 'eng', 'deu', 'nld', 'ind', 'fin', 'hau']\n",
    "    true_clean = clean_stacked[np.isin(clean_stacked[:,0], lang_codes),:] # Remove all rows that aren't our target languages\n",
    "    # print(true_clean.shape)\n",
    "    return true_clean[:,1], true_clean[:,0] # Return cleaned as X and y split again\n",
    "\n",
    "def clean_filter_and_stack(X_train_file, y_train_file, X_test_file, y_test_file):\n",
    "    X_train_clean, y_train_clean = clean_np_data(file_to_np_array(X_train_file, X_train_file), \n",
    "                                       file_to_np_array(y_train_file, y_train_file))\n",
    "    X_test_clean, y_test_clean = clean_np_data(file_to_np_array(X_test_file, X_test_file), \n",
    "                                       file_to_np_array(y_test_file, y_test_file))\n",
    "    return np.hstack((X_train_clean, X_test_clean)), np.hstack((y_train_clean, y_test_clean))\n",
    "\n",
    "X, y = clean_filter_and_stack(\"Data/x_train.txt\", \n",
    "                                      \"Data/y_train.txt\", \n",
    "                                      \"Data/x_test.txt\", \n",
    "                                      \"Data/y_test.txt\")\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d27cb38f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>percent</th>\n",
       "      <th>avg_word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>deu</td>\n",
       "      <td>10.0</td>\n",
       "      <td>59.762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eng</td>\n",
       "      <td>10.0</td>\n",
       "      <td>70.455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fin</td>\n",
       "      <td>10.0</td>\n",
       "      <td>48.431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fra</td>\n",
       "      <td>10.0</td>\n",
       "      <td>67.707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hau</td>\n",
       "      <td>10.0</td>\n",
       "      <td>75.802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ind</td>\n",
       "      <td>10.0</td>\n",
       "      <td>57.147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ita</td>\n",
       "      <td>10.0</td>\n",
       "      <td>68.192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>nld</td>\n",
       "      <td>10.0</td>\n",
       "      <td>55.657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>por</td>\n",
       "      <td>10.0</td>\n",
       "      <td>66.184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>spa</td>\n",
       "      <td>10.0</td>\n",
       "      <td>67.295</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lang  percent  avg_word_count\n",
       "0  deu     10.0          59.762\n",
       "1  eng     10.0          70.455\n",
       "2  fin     10.0          48.431\n",
       "3  fra     10.0          67.707\n",
       "4  hau     10.0          75.802\n",
       "5  ind     10.0          57.147\n",
       "6  ita     10.0          68.192\n",
       "7  nld     10.0          55.657\n",
       "8  por     10.0          66.184\n",
       "9  spa     10.0          67.295"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def summarize_language_data(X, y):\n",
    "    df = pd.DataFrame({'text': X.flatten(), 'lang': y.flatten()})\n",
    "    df['word_count'] = df['text'].apply(lambda x: len(str(x).split()))\n",
    "    \n",
    "    summary = df.groupby('lang').agg(\n",
    "        percent=('lang', lambda x: 100 * len(x) / len(df)),\n",
    "        avg_word_count=('word_count', 'mean')\n",
    "    ).reset_index()\n",
    "    \n",
    "    return summary\n",
    "\n",
    "display(summarize_language_data(X, y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a7d528",
   "metadata": {},
   "source": [
    "#### Data Split\n",
    "Here we use Sklearn train_test_split to split our data into 70/30 train and test splits, respectively, after shuffling them randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "60b758fc-1a04-4cc4-8c94-9c6034e4ca00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7000,) (7000,)\n",
      "(3000,) (3000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=17)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a5925257-f51e-452b-be5b-7a4dede72735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done vectorizing\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(strip_accents='unicode')\n",
    "X_tr_vectors = vectorizer.fit_transform(X_train)\n",
    "X_te_vectors = vectorizer.transform(X_test)\n",
    "print(\"Done vectorizing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb703257-141d-403f-a819-31553f0ccde4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training MNB\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(X_tr_vectors, y_train)\n",
    "print(\"Done training MNB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b8e9320b-e9e6-4217-8dc6-e068913a34f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.981\n",
      "['fra' 'ita' 'fra' 'hau' 'spa' 'por' 'hau' 'eng' 'fra' 'fra']\n",
      "[\"Le parc national de la Chorie, en russe Шорский национальный парк, Chorsky natsionalny park, est une aire naturelle protégée de Russie située en Chorie montagneuse dans la partie méridionale de l'oblast de Kemerovo et fondée le 27 décembre 1989. Il s'étend sur 4 138 kilomètres carrés dans le raïon de Tachtagol. La longueur du parc du nord au sud est de 110 km et sa largeur d'ouest en est de 90 km. Son siège se trouve à Tachtagol.\"\n",
      " \"Concentrare le vetrerie a Murano servì alla Serenissima, gelosa di un'arte che l'aveva resa celebre in tutto il mondo sin dalle origini, a controllarne meglio l'attività. I mastri vetrai erano obbligati a vivere sull'isola e non potevano lasciare Venezia senza un permesso speciale. Molti tuttavia riuscirono a fuggire, esportando all'estero le loro celebri tecniche. La più importante crisi che colpì l'industria fu quella del XV secolo, quando si cominciò la fabbricazione dei cristalli di Boemia, forse ispirati agli stessi vetri di Murano. Venezia ne uscì, specie da quando il vetro fu utilizzato per la realizzazione di lampadari, tutt'oggi tra i manufatti più noti di Murano.\"\n",
      " \"En 2002, le Micrograph F1, descendant du Mikrograph de 1916, remporte le prix de la Montre design au Grand Prix d’Horlogerie de Genève. Le Micrograph F1 est le premier chronographe poignet, automatique mécanique qui mesure les temps au 1/100e de seconde. Cette même année, la société sponsorise l’équipe Oracle BMW, avec Chris Dickson et Peter Holmberg, pour la Coupe de l'America. Pour commémorer ce partenariat, TAG Heuer offre aux fervents de courses nautiques une édition limitée du chronographe Link Searacer Oracle porté par les membres de l’équipe.\"\n",
      " 'Kogin Nijar ko Neja na da tsawon kilomita 4,180. Zurfinta marubba’in kilomita 2,117,700 a kasa. Matsakaicin saurinta 5,589 m3/s wanda ya bambanta daga saurin 500 m3/s zuwa 27,600 m3/s. Mafarinta daga tsaunukan Gine, a kudu maso gabashin Gine. Kananan rafufukanta su ne kogin Bani, kogin Sokoto, kogin Kaduna da na kogin Benuwe. Ta bi cikin Mali, Nijar da Najeriya zuwa Tekun Atalanta wanda ta bi Neja Delta. Waɗannan biranen na samuwa a gefen kogin Neja: Tembakounda, Bamako, Timbuktu, Niamey.'\n",
      " 'Los matrimonios consanguíneos están prohibidos por la ley en muchas partes de Estados Unidos y Europa.[37]\\u200b Mientras que los matrimonios consanguíneos son comunes y culturalmente preferidos en países islámicos y por inmigrantes de países musulmanes en otras partes del mundo, están culturalmente prohibidos o considerados indeseables en la mayoría de las sociedades cristianas, hindúes y budistas.[38]\\u200b Los matrimonios arreglados consanguíneos eran comunes en las comunidades judías hasta el siglo XX, pero se han reducido a menos del 10% en los tiempos modernos.[39]\\u200b[40]\\u200b'\n",
      " 'Em março de 1697, quando Henry Maundrell visitou o poço de Jacó, a profundidade da água no poço media 4,6 m. Edward Robinson visitou o local em meados do século XIX, descrevendo os \"restos da antiga igreja\", encontrando-se um pouco acima do poço para o sudoeste como uma \"massa disforme de ruínas, entre as quais são fragmentos de cinza, colunas de granito, ainda mantendo o seu antigo polimento.\" Os cristãos locais continuaram a venerar o lugar, mesmo quando ele estava sem uma igreja. Em 1860, o local foi adquirido pelo Patriarcado Ortodoxo Grego e uma nova igreja, dedicada à Santa Fotina, foi construída. Em 1927, um terremoto veio destruir o templo.'\n",
      " 'Fiye da 50 gajeren zango na tashar watsa ake amfani da su rufe mafi yawansu duniya. an watsa shirye-shirye via internet da yawa da tauraron dan adam. da shirye-shiryen da ake rebroadcast da yawa na gida FM da kuma AM gidajen rediyo a duk duniya.'\n",
      " 'From 1932 to 1940, Johnson openly sympathized with the extreme populist movements of Huey Long, Governor of Louisiana, and Father Charles Coughlin. As a correspondent for Coughlin\\'s newspaper Social Justice, he made several trips to Germany, sympathetically covering the huge Nazi rally at Nuremberg and the German invasion of Poland in 1939. The American correspondent William Shirer who also covered the German invasion of Poland, noted his enthusiasm for the Germans and called him \"The American fascist.\" However, there is no other evidence that he was a fascist or member of the Nazi Party. He actively helped refugees from Germany, including Walter Gropius and Ludwig Mies van der Rohe, to emigrate and become established in the United States. He later described his activities in Germany as \"stupid.\" He was embarrassed, many years after the war, when Hitler\\'s architect, Albert Speer, who admired Johnson\\'s work, sent a copy of his own book as a gift to Johnson. The FBI investigated him for his past contacts with the German government but he was not found to be a threat and he served honorably in the military. Regarding this period in his life, he later said, \"I have no excuse (for) such unbelievable stupidity... I don\\'t know how you expiate guilt.\" In 1956, donated his design for a building of worship to what is now one of the country\\'s oldest Jewish congregations, Congregation Kneses Tifereth Israel in Port Chester, New York. According to one source \"all critics agree that his design of the Port Chester Synagogue can be considered as his attempt to ask for forgiveness\" for his admitted \"stupidity\". The building, which stands today, is a \"crisp juxtaposition of geometric forms\"'\n",
      " \"La commune est limitrophe de Gonesse, Le Thillay, Vaudherland, Goussainville, Louvres et Épiais-lès-Louvres dans le département du Val-d'Oise, du Tremblay-en-France dans le département de la Seine-Saint-Denis et de Mauregard dans le département de Seine-et-Marne.\"\n",
      " \"Une vanne peut être automatisée, par le biais d'un servomoteur (un élément assurant la conversion d'un signal de commande en mouvement de la vanne).\"]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = model.predict(X_te_vectors)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(y_pred[0:10])\n",
    "print(X_test[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f966d3e-57e4-4b15-9897-32ca8d142f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Results (Accuracy):\n",
      "  Fold 1: 98.50%\n",
      "  Fold 2: 99.30%\n",
      "  Fold 3: 98.80%\n",
      "  Fold 4: 99.10%\n",
      "  Fold 5: 98.60%\n",
      "Mean Accuracy: 98.86%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "model = make_pipeline(CountVectorizer(), MultinomialNB())\n",
    "\n",
    "num_folds = 5\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=17)\n",
    "\n",
    "cross_val_results = cross_val_score(model, X.flatten(), y.flatten(), cv=kf)\n",
    "\n",
    "print(\"Cross-Validation Results (Accuracy):\")\n",
    "for i, result in enumerate(cross_val_results, 1):\n",
    "    print(f\"  Fold {i}: {result * 100:.2f}%\")\n",
    "\n",
    "print(f'Mean Accuracy: {cross_val_results.mean() * 100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
