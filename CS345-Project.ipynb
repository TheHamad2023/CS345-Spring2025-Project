{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4315aa3e",
   "metadata": {},
   "source": [
    "# CS345 Project\n",
    "\n",
    "## Team Members\n",
    "1. Hamad Alyami\n",
    "2. Benito Encarnacion\n",
    "\n",
    "## Dataset\n",
    "Our dataset was from Kaggle by a user called Mexwell. The data is paragraphs scraped from wikipedia in 2018 in 235 languages.\n",
    "\n",
    "The dataset contains 235,000 datasets with balance between language proportions and a test and train split provided.\n",
    "\n",
    "The downloaded folder from Kaggle contains:\n",
    "- labels.csv: A file containing the language name, 2-3 letter code, German name, and language family of all the languages present in the dataset.\n",
    "- README.txt: A file explaining the folder contents.\n",
    "- urls.txt: A file containing the urls of where the paragraphs were found.\n",
    "- x_test.txt: The testing data samples, paragraphs in multiple languages.\n",
    "- x_train.txt: The training data samples, paragraphs in multiple langauges\n",
    "- y_test.txt: The labels for the testing dataset, using the 2-3 letter codes found in labels.csv.\n",
    "- y_train.txt: The labels for the training dataset, using the 2-3 letter codes found in labels.csv\n",
    "\n",
    "\n",
    "## Project\n",
    "Our project is to train and compare two ML models on the Latin Alphabet languages present in the dataset and compare their performance.\n",
    "\n",
    "## Motivation\n",
    "We decided to do this project because it allows us to explore practical applications of natural language processing and machine learning by working with real-world multilingual data. Language identification is an important task in many systems and applications like search engines, translation tools, and content moderation. Working with such a dataset gives us the opportunity to apply classification techniques in a meaningful way. By focusing on languages that use the Latin alphabet, we avoid complications from different writing systems while still working with a variety of languages.\n",
    "\n",
    "## Models\n",
    "The models we decided to work with in this project are:\n",
    "- Multinomial Naive-Bayes (MNB): Uses word frequencies in each class, langauges in our case, to guess the most likely class for text it has not seen.\n",
    "\n",
    "- Feed Forward Neural Network (FNN): An artificial Neural Network where information moves from input to output without looping back. It uses neurons, connected nodes, to learn patterns and make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1704ce66",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "We will begin by reading the data from the files then:\n",
    "1. Remove Null Values\n",
    "2. Filter to keep texts of languages we want using the 2-3 letter codes\n",
    "3. Return both samples from x_test and x_train and labels from y_test and y_train stacked into X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7602a789-a084-4f24-b977-47df8039c2a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data/x_train.txt: Read!\n",
      "Data/y_train.txt: Read!\n",
      "Data/x_test.txt: Read!\n",
      "Data/y_test.txt: Read!\n",
      "(5000,) (5000,)\n"
     ]
    }
   ],
   "source": [
    "#Understanding the data set\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def file_to_np_array(path, label):\n",
    "    try:\n",
    "        df = pd.read_csv(path, sep='<NonExistenceSeparator>', header=None, engine='python')\n",
    "        print(f\"{label}: Read!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading the {label} file: {e}\")\n",
    "        return None\n",
    "    return df.to_numpy()\n",
    "\n",
    "\n",
    "def clean_np_data(X, y):\n",
    "    stacked = np.hstack((y, X)) # Stack y and X side by side\n",
    "    # print(stacked.shape)\n",
    "    clean_stacked = stacked[~np.any(pd.isna(stacked), axis=1), :] # Remove empty values\n",
    "    # print(clean_stacked.shape)\n",
    "    lang_codes = ['ita', 'fra', 'eng', 'ind', 'spa']\n",
    "    true_clean = clean_stacked[np.isin(clean_stacked[:,0], lang_codes),:] # Remove all rows that aren't our target languages\n",
    "    # print(true_clean.shape)\n",
    "    return true_clean[:,1], true_clean[:,0] # Return cleaned as X and y split again\n",
    "\n",
    "def clean_filter_and_stack(X_train_file, y_train_file, X_test_file, y_test_file):\n",
    "    X_train_clean, y_train_clean = clean_np_data(file_to_np_array(X_train_file, X_train_file), \n",
    "                                       file_to_np_array(y_train_file, y_train_file))\n",
    "    X_test_clean, y_test_clean = clean_np_data(file_to_np_array(X_test_file, X_test_file), \n",
    "                                       file_to_np_array(y_test_file, y_test_file))\n",
    "    return np.hstack((X_train_clean, X_test_clean)), np.hstack((y_train_clean, y_test_clean))\n",
    "\n",
    "X, y = clean_filter_and_stack(\"Data/x_train.txt\", \n",
    "                                      \"Data/y_train.txt\", \n",
    "                                      \"Data/x_test.txt\", \n",
    "                                      \"Data/y_test.txt\")\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d27cb38f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>percent</th>\n",
       "      <th>avg_word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>eng</td>\n",
       "      <td>20.0</td>\n",
       "      <td>70.455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fra</td>\n",
       "      <td>20.0</td>\n",
       "      <td>67.707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ind</td>\n",
       "      <td>20.0</td>\n",
       "      <td>57.147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ita</td>\n",
       "      <td>20.0</td>\n",
       "      <td>68.192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spa</td>\n",
       "      <td>20.0</td>\n",
       "      <td>67.295</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lang  percent  avg_word_count\n",
       "0  eng     20.0          70.455\n",
       "1  fra     20.0          67.707\n",
       "2  ind     20.0          57.147\n",
       "3  ita     20.0          68.192\n",
       "4  spa     20.0          67.295"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def summarize_language_data(X, y):\n",
    "    df = pd.DataFrame({'text': X.flatten(), 'lang': y.flatten()})\n",
    "    df['word_count'] = df['text'].apply(lambda x: len(str(x).split()))\n",
    "    \n",
    "    summary = df.groupby('lang').agg(\n",
    "        percent=('lang', lambda x: 100 * len(x) / len(df)),\n",
    "        avg_word_count=('word_count', 'mean')\n",
    "    ).reset_index()\n",
    "    \n",
    "    return summary\n",
    "\n",
    "display(summarize_language_data(X, y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a7d528",
   "metadata": {},
   "source": [
    "#### Data Split\n",
    "Here we use Sklearn train_test_split to split our data into 70/30 train and test splits, respectively, after shuffling them randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60b758fc-1a04-4cc4-8c94-9c6034e4ca00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3500,) (3500,)\n",
      "(1500,) (1500,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=17)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5925257-f51e-452b-be5b-7a4dede72735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done vectorizing\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(strip_accents='unicode')\n",
    "X_tr_vectors = vectorizer.fit_transform(X_train)\n",
    "X_te_vectors = vectorizer.transform(X_test)\n",
    "print(\"Done vectorizing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb703257-141d-403f-a819-31553f0ccde4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training MNB\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(X_tr_vectors, y_train)\n",
    "print(\"Done training MNB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8e9320b-e9e6-4217-8dc6-e068913a34f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9893333333333333\n",
      "['fra' 'fra' 'ind' 'ind' 'spa' 'spa' 'ita' 'eng' 'ita' 'eng']\n",
      "['Les magnolias ou magnoliers (Magnolia L., 1753) forment un genre de plantes à fleurs, de la famille des magnoliacées, qui comprend environ cent dix espèces, essentiellement des arbres et arbustes, des régions tempérées chaudes.'\n",
      " 'Honoré Beaugrand, fils de Louis Beaugrand, dit Champagne, navigateur, et de Marie-Josephte (Joséphine) Marion, est l’aîné des fils d’une famille qui comptera six enfants (3 garçons et 3 filles).'\n",
      " 'Sesudah itu, masih juga terdapat penyerangan-penyerangan, seperti ke Avignon (tahun 734), ke Lyon (tahun 743), dan pulau-pulau yang terdapat di Laut Tengah, Mallorca, Korsika, Sardegna, Kreta, Rhodos, Siprus dan sebagian dari Sisilia juga jatuh ke tangan Islam pada zaman Bani Umayyah. Gelombang kedua terbesar dari penyerbuan kaum Muslimin yang geraknya dimulai pada permulaan abad ke-8 M ini, telah menjangkau seluruh Spanyol dan melebar jauh menjangkau Perancis Tengah dan bagian-bagian penting dari Italia.'\n",
      " 'Paspor ini berisi 24 atau 48 halaman dan berlaku selama 5 tahun. Namun paspor yang diterbitkan oleh perwakilan RI di luar negeri lazimnya menerbitkan paspor dengan jangka waktu 3 tahun dan dapat diperpanjang 2 tahun setelahnya.'\n",
      " 'Estableció las bases de la historia moderna de la cultura portuguesa, que alcanzó gran proyección nacional, especialmente a través de los textos dedicados a Platón, Hegel, Husserl y especialmente, Baruch Spinoza, de quien se convirtió en un experto de renombre internacional.'\n",
      " 'Uno puede encontrar más epítetos en la alabanza de este artículo que Turguénev una vez ensambló para elogiar la lengua rusa, o Nekrásov para alabar a la madre Rusia: grande, poderoso, abundante, altamente ramificado, multiforme, de barrido amplio 58, que resumía el mundo no tanto a través de los términos exactos de sus secciones como en su extendida interpretación dialéctica.'\n",
      " 'Il 9 luglio 1998 muore Aldo Stellita: fondatore, bassista e autore della maggior parte dei testi delle canzoni del gruppo. Pochi mesi dopo anche Laura Valente e Sergio Cossu lasciano definitivamente il gruppo; queste uscite determinano la temporanea \"fine\" dei Matia Bazar dopo la scomparsa di Aldo.'\n",
      " 'Hamilton has been the primary songwriter, guitarist, and vocalist for Brothers Past, as well as co-producer for all of their recorded releases.'\n",
      " 'Ernst Theodor Amadeus Hoffmann, meglio noto come E. T. A. Hoffmann (Königsberg, 24 gennaio 1776 – Berlino, 25 giugno 1822), è stato uno scrittore, compositore, pittore e giurista tedesco, esponente del Romanticismo.'\n",
      " 'At about the same time as the AT&T Building, Johnson and Burgee completed other remarkable postmodern skyscrapers; the Bank of America Center (Formerly Republic Bank Center) in Houston (1983) and the PPG Place, the headquarters of the Pittsburgh Plate Glass company (1979–84). Both buildings combined modern materials, construction and scale with suggestions of traditional architecture. The forms of PPG Place suggested the neogothic tower of the Houses of Parliament in London, while the Bank of America Center appeared inspired, on a colossal scale, the stepped houses of Flemish Renaissance architecture.']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = model.predict(X_te_vectors)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(y_pred[0:10])\n",
    "print(X_test[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f966d3e-57e4-4b15-9897-32ca8d142f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Results (Accuracy):\n",
      "  Fold 1: 98.50%\n",
      "  Fold 2: 99.30%\n",
      "  Fold 3: 98.80%\n",
      "  Fold 4: 99.10%\n",
      "  Fold 5: 98.60%\n",
      "Mean Accuracy: 98.86%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "model = make_pipeline(CountVectorizer(), MultinomialNB())\n",
    "\n",
    "num_folds = 5\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=17)\n",
    "\n",
    "cross_val_results = cross_val_score(model, X.flatten(), y.flatten(), cv=kf)\n",
    "\n",
    "print(\"Cross-Validation Results (Accuracy):\")\n",
    "for i, result in enumerate(cross_val_results, 1):\n",
    "    print(f\"  Fold {i}: {result * 100:.2f}%\")\n",
    "\n",
    "print(f'Mean Accuracy: {cross_val_results.mean() * 100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
