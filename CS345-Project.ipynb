{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4315aa3e",
   "metadata": {},
   "source": [
    "# CS345 Project\n",
    "\n",
    "## Team Members\n",
    "1. Hamad Alyami\n",
    "2. Benito Encarnacion\n",
    "\n",
    "## Dataset\n",
    "Our dataset was from Kaggle by a user called Mexwell. The data is paragraphs scraped from wikipedia in 2018 in 235 languages.\n",
    "\n",
    "The dataset contains 235,000 datasets with balance between language proportions and a test and train split provided.\n",
    "\n",
    "The downloaded folder from Kaggle contains:\n",
    "- labels.csv: A file containing the language name, 2-3 letter code, German name, and language family of all the languages present in the dataset.\n",
    "- README.txt: A file explaining the folder contents.\n",
    "- urls.txt: A file containing the urls of where the paragraphs were found.\n",
    "- x_test.txt: The testing data samples, paragraphs in multiple languages.\n",
    "- x_train.txt: The training data samples, paragraphs in multiple langauges\n",
    "- y_test.txt: The labels for the testing dataset, using the 2-3 letter codes found in labels.csv.\n",
    "- y_train.txt: The labels for the training dataset, using the 2-3 letter codes found in labels.csv\n",
    "\n",
    "\n",
    "## Project\n",
    "Our project is to train and compare two ML models on the Latin Alphabet languages present in the dataset and compare their performance.\n",
    "\n",
    "## Motivation\n",
    "We decided to do this project because it allows us to explore practical applications of natural language processing and machine learning by working with real-world multilingual data. Language identification is an important task in many systems and applications like search engines, translation tools, and content moderation. Working with such a dataset gives us the opportunity to apply classification techniques in a meaningful way. By focusing on languages that use the Latin alphabet, we avoid complications from different writing systems while still working with a variety of languages.\n",
    "\n",
    "## Models\n",
    "The models we decided to work with in this project are:\n",
    "- Multinomial Naive-Bayes (MNB): Uses word frequencies in each class, langauges in our case, to guess the most likely class for text it has not seen.\n",
    "\n",
    "- Feed Forward Neural Network (FNN): An artificial Neural Network where information moves from input to output without looping back. It uses neurons, connected nodes, to learn patterns and make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1704ce66",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "We will begin by reading the data from the files then:\n",
    "1. Remove Null Values\n",
    "2. Filter to keep texts of languages we want using the 2-3 letter codes\n",
    "3. Return both samples from x_test and x_train and labels from y_test and y_train stacked into X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7602a789-a084-4f24-b977-47df8039c2a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data/x_train.txt: Read!\n",
      "Data/y_train.txt: Read!\n",
      "Data/x_test.txt: Read!\n",
      "Data/y_test.txt: Read!\n",
      "(10000,) (10000,)\n"
     ]
    }
   ],
   "source": [
    "#Understanding the data set\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "            #Italina, French, Spanish, Portugese, English, German, Dutch, Indonesian, Finnish, Hausa\n",
    "lang_codes = ['ita', 'fra', 'spa', 'por', 'eng', 'deu', 'nld', 'ind', 'fin', 'hau']\n",
    "langs = ['Italian', 'French', 'Spanish', 'Portuguese', 'English', 'German', 'Dutch', 'Indonesian', 'Finnish', 'Hausa']\n",
    "\n",
    "def file_to_np_array(path, label):\n",
    "    try:\n",
    "        df = pd.read_csv(path, sep='<NonExistenceSeparator>', header=None, engine='python')\n",
    "        print(f\"{label}: Read!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading the {label} file: {e}\")\n",
    "        return None\n",
    "    return df.to_numpy()\n",
    "\n",
    "\n",
    "def clean_np_data(X, y):\n",
    "    stacked = np.hstack((y, X)) # Stack y and X side by side\n",
    "    # print(stacked.shape)\n",
    "    clean_stacked = stacked[~np.any(pd.isna(stacked), axis=1), :] # Remove empty values\n",
    "    # print(clean_stacked.shape)\n",
    "    true_clean = clean_stacked[np.isin(clean_stacked[:,0], lang_codes),:] # Remove all rows that aren't our target languages\n",
    "    # print(true_clean.shape)\n",
    "    return true_clean[:,1], true_clean[:,0] # Return cleaned as X and y split again\n",
    "\n",
    "def clean_filter_and_stack(X_train_file, y_train_file, X_test_file, y_test_file):\n",
    "    X_train_clean, y_train_clean = clean_np_data(file_to_np_array(X_train_file, X_train_file), \n",
    "                                       file_to_np_array(y_train_file, y_train_file))\n",
    "    X_test_clean, y_test_clean = clean_np_data(file_to_np_array(X_test_file, X_test_file), \n",
    "                                       file_to_np_array(y_test_file, y_test_file))\n",
    "    return np.hstack((X_train_clean, X_test_clean)), np.hstack((y_train_clean, y_test_clean))\n",
    "\n",
    "X, y = clean_filter_and_stack(\"Data/x_train.txt\", \n",
    "                                      \"Data/y_train.txt\", \n",
    "                                      \"Data/x_test.txt\", \n",
    "                                      \"Data/y_test.txt\")\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27cb38f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Language</th>\n",
       "      <th>Percent of Dataset</th>\n",
       "      <th>Average Word Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Italian</td>\n",
       "      <td>10.0</td>\n",
       "      <td>68.192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>French</td>\n",
       "      <td>10.0</td>\n",
       "      <td>67.707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Spanish</td>\n",
       "      <td>10.0</td>\n",
       "      <td>67.295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Portuguese</td>\n",
       "      <td>10.0</td>\n",
       "      <td>66.184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>English</td>\n",
       "      <td>10.0</td>\n",
       "      <td>70.455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>German</td>\n",
       "      <td>10.0</td>\n",
       "      <td>59.762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Dutch</td>\n",
       "      <td>10.0</td>\n",
       "      <td>55.657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Indonesian</td>\n",
       "      <td>10.0</td>\n",
       "      <td>57.147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Finnish</td>\n",
       "      <td>10.0</td>\n",
       "      <td>48.431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Hausa</td>\n",
       "      <td>10.0</td>\n",
       "      <td>75.802</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Language  Percent of Dataset  Average Word Count\n",
       "0     Italian                10.0              68.192\n",
       "1      French                10.0              67.707\n",
       "2     Spanish                10.0              67.295\n",
       "3  Portuguese                10.0              66.184\n",
       "4     English                10.0              70.455\n",
       "5      German                10.0              59.762\n",
       "6       Dutch                10.0              55.657\n",
       "7  Indonesian                10.0              57.147\n",
       "8     Finnish                10.0              48.431\n",
       "9       Hausa                10.0              75.802"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def avg_words(filtered_X):\n",
    "    total = 0\n",
    "    for text in filtered_X:\n",
    "        words = str(text).split()\n",
    "        total += len(words)\n",
    "\n",
    "    return total / len(filtered_X)\n",
    "\n",
    "def word_count_perlang(X, y):\n",
    "    avg_word_count = []\n",
    "    for lang in lang_codes:\n",
    "        filtered_X = X[y == lang]\n",
    "        avg_word_count.append(avg_words(filtered_X))\n",
    "    \n",
    "    return avg_word_count\n",
    "\n",
    "def lang_perc(y):\n",
    "    lang_perc = []\n",
    "    total = len(y)\n",
    "    for lang in lang_codes:\n",
    "        count = (y == lang).sum()\n",
    "        percent = (count / total) * 100\n",
    "        lang_perc.append(percent)\n",
    "    return lang_perc\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Language': langs,\n",
    "    'Percent of Dataset': lang_perc(y),\n",
    "    'Average Word Count': word_count_perlang(X, y)\n",
    "})\n",
    "\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a7d528",
   "metadata": {},
   "source": [
    "#### Data Split\n",
    "Here we use Sklearn train_test_split to split our data into 70/30 train and test splits, respectively, after shuffling them randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "60b758fc-1a04-4cc4-8c94-9c6034e4ca00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7000,) (7000,)\n",
      "(3000,) (3000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=17)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a5925257-f51e-452b-be5b-7a4dede72735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done vectorizing\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(strip_accents='unicode')\n",
    "X_tr_vectors = vectorizer.fit_transform(X_train)\n",
    "X_te_vectors = vectorizer.transform(X_test)\n",
    "print(\"Done vectorizing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eb703257-141d-403f-a819-31553f0ccde4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training MNB\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(X_tr_vectors, y_train)\n",
    "print(\"Done training MNB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b8e9320b-e9e6-4217-8dc6-e068913a34f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.981\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = model.predict(X_te_vectors)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "# print(y_pred[0:10])\n",
    "# print(X_test[0:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d6603099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy per language:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167915/230907993.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  lang_accuracy = df.groupby('language').apply(group_accuracy).reset_index(name='accuracy')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>deu</td>\n",
       "      <td>0.975694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eng</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fin</td>\n",
       "      <td>0.993151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fra</td>\n",
       "      <td>0.993355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hau</td>\n",
       "      <td>0.996622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ind</td>\n",
       "      <td>0.986159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ita</td>\n",
       "      <td>0.968553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>nld</td>\n",
       "      <td>0.982759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>por</td>\n",
       "      <td>0.937282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>spa</td>\n",
       "      <td>0.976471</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  language  accuracy\n",
       "0      deu  0.975694\n",
       "1      eng  1.000000\n",
       "2      fin  0.993151\n",
       "3      fra  0.993355\n",
       "4      hau  0.996622\n",
       "5      ind  0.986159\n",
       "6      ita  0.968553\n",
       "7      nld  0.982759\n",
       "8      por  0.937282\n",
       "9      spa  0.976471"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def group_accuracy(group):\n",
    "    return accuracy_score(group['language'], group['pred'])\n",
    "\n",
    "df = pd.DataFrame({'language': y_test, 'pred': y_pred})\n",
    "\n",
    "lang_accuracy = df.groupby('language').apply(group_accuracy).reset_index(name='accuracy')\n",
    "\n",
    "print(\"\\nAccuracy per language:\")\n",
    "# print(lang_accuracy)\n",
    "display(lang_accuracy)\n",
    "\n",
    "# df = pd.DataFrame({'true_label': y_test, 'pred': y_pred})\n",
    "# df['correct'] = (df['true_label'] == df['pred']).astype(int)\n",
    "\n",
    "# lang_accuracy = df.groupby('true_label')['correct'].mean()\n",
    "\n",
    "# print(\"\\nAccuracy per language:\")\n",
    "# print(lang_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2f966d3e-57e4-4b15-9897-32ca8d142f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Results (Accuracy):\n",
      "  Fold 1: 98.25%\n",
      "  Fold 2: 98.30%\n",
      "  Fold 3: 98.25%\n",
      "  Fold 4: 98.60%\n",
      "  Fold 5: 98.40%\n",
      "Mean Accuracy: 98.36%\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Requested level (accuracy) does not match index name (true)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcorrect\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Group by language and compute mean accuracy across all folds\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m lang_accuracy \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcorrect\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mreset_index(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMean Accuracy per Language Across All Folds:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     30\u001b[0m display(lang_accuracy)\n",
      "File \u001b[0;32m~/development/python/tools/anaconda3/lib/python3.12/site-packages/pandas/core/series.py:1770\u001b[0m, in \u001b[0;36mSeries.reset_index\u001b[0;34m(self, level, drop, name, inplace, allow_duplicates)\u001b[0m\n\u001b[1;32m   1767\u001b[0m             name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\n\u001b[1;32m   1769\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame(name)\n\u001b[0;32m-> 1770\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\u001b[38;5;241m.\u001b[39mreset_index(\n\u001b[1;32m   1771\u001b[0m         level\u001b[38;5;241m=\u001b[39mlevel, drop\u001b[38;5;241m=\u001b[39mdrop, allow_duplicates\u001b[38;5;241m=\u001b[39mallow_duplicates\n\u001b[1;32m   1772\u001b[0m     )\n\u001b[1;32m   1773\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/development/python/tools/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py:6425\u001b[0m, in \u001b[0;36mDataFrame.reset_index\u001b[0;34m(self, level, drop, inplace, col_level, col_fill, allow_duplicates, names)\u001b[0m\n\u001b[1;32m   6423\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(level, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[1;32m   6424\u001b[0m     level \u001b[38;5;241m=\u001b[39m [level]\n\u001b[0;32m-> 6425\u001b[0m level \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39m_get_level_number(lev) \u001b[38;5;28;01mfor\u001b[39;00m lev \u001b[38;5;129;01min\u001b[39;00m level]\n\u001b[1;32m   6426\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(level) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mnlevels:\n\u001b[1;32m   6427\u001b[0m     new_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mdroplevel(level)\n",
      "File \u001b[0;32m~/development/python/tools/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:2017\u001b[0m, in \u001b[0;36mIndex._get_level_number\u001b[0;34m(self, level)\u001b[0m\n\u001b[1;32m   2016\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_level_number\u001b[39m(\u001b[38;5;28mself\u001b[39m, level) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m-> 2017\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_index_level(level)\n\u001b[1;32m   2018\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/development/python/tools/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:2012\u001b[0m, in \u001b[0;36mIndex._validate_index_level\u001b[0;34m(self, level)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\n\u001b[1;32m   2009\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mToo many levels: Index has only 1 level, not \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlevel\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2010\u001b[0m         )\n\u001b[1;32m   2011\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m level \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname:\n\u001b[0;32m-> 2012\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[1;32m   2013\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequested level (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlevel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) does not match index name (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2014\u001b[0m     )\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Requested level (accuracy) does not match index name (true)'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "model = make_pipeline(CountVectorizer(), MultinomialNB())\n",
    "\n",
    "num_folds = 5\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=17)\n",
    "\n",
    "cross_val_results = cross_val_score(model, X.flatten(), y.flatten(), cv=kf)\n",
    "\n",
    "all_preds = []\n",
    "all_true = []\n",
    "\n",
    "print(\"Cross-Validation Results (Accuracy):\")\n",
    "for i, result in enumerate(cross_val_results, 1):\n",
    "    print(f\"  Fold {i}: {result * 100:.2f}%\")\n",
    "\n",
    "    all_preds.extend(y_pred)\n",
    "    all_true.extend(y_test)\n",
    "\n",
    "print(f'Mean Accuracy: {cross_val_results.mean() * 100:.2f}%')\n",
    "\n",
    "df = pd.DataFrame({'true': all_true, 'pred': all_preds})\n",
    "df['correct'] = (df['true'] == df['pred']).astype(int)\n",
    "\n",
    "# Group by language and compute mean accuracy across all folds\n",
    "lang_accuracy = df.groupby('true')['correct'].mean().reset_index('accuracy')\n",
    "\n",
    "print(\"\\nMean Accuracy per Language Across All Folds:\")\n",
    "display(lang_accuracy)\n",
    "\n",
    "# print(lang_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "81a93297-6dd5-4cb8-af86-0ff9a0616fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorganizing data for Feed Forward Neural Network input\n",
    "\n",
    "# 1) convert string labels into integer labels\n",
    "#      and make func to convert back\n",
    "\n",
    "all_str_labels = ['ita', 'fra', 'spa', 'por', 'eng', 'deu', 'nld', 'ind', 'fin', 'hau']\n",
    "\n",
    "def str_labels_to_int_labels(labelArr, string_labels):\n",
    "    rtn = np.empty(labelArr.shape, dtype=int)\n",
    "    for i, v in enumerate(string_labels):\n",
    "        rtn[labelArr == v] = i\n",
    "    return rtn\n",
    "\n",
    "def int_labels_to_str_labels(labelArr, string_labels):\n",
    "    rtn = np.empty(labelArr.shape, dtype='object')\n",
    "    for i, v in enumerate(string_labels):\n",
    "        rtn[labelArr == i] = v\n",
    "    return rtn\n",
    "\n",
    "# print(y_test[0:5])\n",
    "# y1 = str_labels_to_int_labels(y_test, all_str_labels)\n",
    "# print(y1[0:5])\n",
    "# y2 = int_labels_to_str_labels(y1, all_str_labels)\n",
    "# print(y2[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1be68002-eb66-461c-94b4-7fa37cc9e4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) convert data into multi-column matrix of characters\n",
    "#      and make func to convert back\n",
    "\n",
    "def str_vec_to_float_matrix(strVec, longest_str_len):\n",
    "    # Pad strings to all be equal length\n",
    "    padded_strVec = np.char.ljust(strVec, longest_str_len, fillchar=' ')\n",
    "\n",
    "    # turn vector of strings into matrix of characters\n",
    "    stacked_char_matrix = np.vstack([np.array(list(s)) for s in padded_strVec])\n",
    "\n",
    "    # turn char matrix into int matrix\n",
    "    char_matrix_to_int_matrix = np.vectorize(ord)\n",
    "    int_matrix = char_matrix_to_int_matrix(stacked_char_matrix)\n",
    "\n",
    "    #normalize and scale so each value is a float between 0 and 1\n",
    "    matrix_max = np.max(int_matrix)\n",
    "    matrix_min = np.min(int_matrix)\n",
    "    min_subtracted_matrix = int_matrix - matrix_min\n",
    "    normalized_matrix = (min_subtracted_matrix / (matrix_max - matrix_min))\n",
    "    return normalized_matrix\n",
    "\n",
    "# Don't need to convert matrices back into rows of text because the neural network isn't designed to generate anything, just classify\n",
    "# def int_matrix_to_str_vec(intMatrix):\n",
    "#     int_matrix_to_char_matrix = np.vectorize(chr)\n",
    "#     char_matrix = int_matrix_to_char_matrix(intMatrix)\n",
    "#     padded_strVec = np.array([\"\".join(r) for r in char_matrix])\n",
    "#     return np.char.rstrip(padded_strVec)\n",
    "\n",
    "# print(X_test[0])\n",
    "# X_x = str_vec_to_int_matrix(X_test.astype(str))\n",
    "# print(X_x[0])\n",
    "# X_y = int_matrix_to_str_vec(X_x)\n",
    "# print(X_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "58228e9e-a643-4b73-a51a-ba3366793109",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 15\u001b[0m\n\u001b[1;32m      9\u001b[0m     Xe_rtn \u001b[38;5;241m=\u001b[39m str_vec_to_float_matrix(Xe\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m), max_str_len)\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (Xr_rtn, \n\u001b[1;32m     11\u001b[0m             Xe_rtn,\n\u001b[1;32m     12\u001b[0m             str_labels_to_int_labels(yr, all_string_labels), \n\u001b[1;32m     13\u001b[0m             str_labels_to_int_labels(ye, all_string_labels))\n\u001b[0;32m---> 15\u001b[0m (X_tr_nn, X_te_nn, y_tr_nn, y_te_nn) \u001b[38;5;241m=\u001b[39m convert_to_FFNN_format(X_train, X_test, y_train, y_test)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone converting data into FFNN format\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[38], line 8\u001b[0m, in \u001b[0;36mconvert_to_FFNN_format\u001b[0;34m(Xr, Xe, yr, ye)\u001b[0m\n\u001b[1;32m      6\u001b[0m max_str_len_2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(np\u001b[38;5;241m.\u001b[39mchar\u001b[38;5;241m.\u001b[39mstr_len(Xe\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)))\n\u001b[1;32m      7\u001b[0m max_str_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(max_str_len_1, max_str_len_2)\n\u001b[0;32m----> 8\u001b[0m Xr_rtn \u001b[38;5;241m=\u001b[39m str_vec_to_float_matrix(Xr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m), max_str_len)\n\u001b[1;32m      9\u001b[0m Xe_rtn \u001b[38;5;241m=\u001b[39m str_vec_to_float_matrix(Xe\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m), max_str_len)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (Xr_rtn, \n\u001b[1;32m     11\u001b[0m         Xe_rtn,\n\u001b[1;32m     12\u001b[0m         str_labels_to_int_labels(yr, all_string_labels), \n\u001b[1;32m     13\u001b[0m         str_labels_to_int_labels(ye, all_string_labels))\n",
      "Cell \u001b[0;32mIn[37], line 13\u001b[0m, in \u001b[0;36mstr_vec_to_float_matrix\u001b[0;34m(strVec, longest_str_len)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# turn char matrix into int matrix\u001b[39;00m\n\u001b[1;32m     12\u001b[0m char_matrix_to_int_matrix \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvectorize(\u001b[38;5;28mord\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m int_matrix \u001b[38;5;241m=\u001b[39m char_matrix_to_int_matrix(stacked_char_matrix)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#normalize and scale so each value is a float between 0 and 1\u001b[39;00m\n\u001b[1;32m     16\u001b[0m matrix_max \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(int_matrix)\n",
      "File \u001b[0;32m~/development/python/tools/anaconda3/lib/python3.12/site-packages/numpy/lib/function_base.py:2372\u001b[0m, in \u001b[0;36mvectorize.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2369\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_stage_2(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   2370\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m-> 2372\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_as_normal(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/development/python/tools/anaconda3/lib/python3.12/site-packages/numpy/lib/function_base.py:2365\u001b[0m, in \u001b[0;36mvectorize._call_as_normal\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2362\u001b[0m     vargs \u001b[38;5;241m=\u001b[39m [args[_i] \u001b[38;5;28;01mfor\u001b[39;00m _i \u001b[38;5;129;01min\u001b[39;00m inds]\n\u001b[1;32m   2363\u001b[0m     vargs\u001b[38;5;241m.\u001b[39mextend([kwargs[_n] \u001b[38;5;28;01mfor\u001b[39;00m _n \u001b[38;5;129;01min\u001b[39;00m names])\n\u001b[0;32m-> 2365\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vectorize_call(func\u001b[38;5;241m=\u001b[39mfunc, args\u001b[38;5;241m=\u001b[39mvargs)\n",
      "File \u001b[0;32m~/development/python/tools/anaconda3/lib/python3.12/site-packages/numpy/lib/function_base.py:2458\u001b[0m, in \u001b[0;36mvectorize._vectorize_call\u001b[0;34m(self, func, args)\u001b[0m\n\u001b[1;32m   2455\u001b[0m outputs \u001b[38;5;241m=\u001b[39m ufunc(\u001b[38;5;241m*\u001b[39minputs)\n\u001b[1;32m   2457\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ufunc\u001b[38;5;241m.\u001b[39mnout \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m-> 2458\u001b[0m     res \u001b[38;5;241m=\u001b[39m asanyarray(outputs, dtype\u001b[38;5;241m=\u001b[39motypes[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m   2459\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2460\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m([asanyarray(x, dtype\u001b[38;5;241m=\u001b[39mt)\n\u001b[1;32m   2461\u001b[0m                  \u001b[38;5;28;01mfor\u001b[39;00m x, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(outputs, otypes)])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 3) use them both\n",
    "\n",
    "def convert_to_FFNN_format(Xr, Xe, yr, ye):\n",
    "    all_string_labels = ['ita', 'fra', 'spa', 'por', 'eng', 'deu', 'nld', 'ind', 'fin', 'hau']\n",
    "    max_str_len_1 = np.max(np.char.str_len(Xr.astype(str)))\n",
    "    max_str_len_2 = np.max(np.char.str_len(Xe.astype(str)))\n",
    "    max_str_len = max(max_str_len_1, max_str_len_2)\n",
    "    Xr_rtn = str_vec_to_float_matrix(Xr.astype(str), max_str_len)\n",
    "    Xe_rtn = str_vec_to_float_matrix(Xe.astype(str), max_str_len)\n",
    "    return (Xr_rtn, \n",
    "            Xe_rtn,\n",
    "            str_labels_to_int_labels(yr, all_string_labels), \n",
    "            str_labels_to_int_labels(ye, all_string_labels))\n",
    "\n",
    "(X_tr_nn, X_te_nn, y_tr_nn, y_te_nn) = convert_to_FFNN_format(X_train, X_test, y_train, y_test)\n",
    "\n",
    "print(\"Done converting data into FFNN format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c436646e-e772-42a2-9b08-ba6d866aaecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7000, 5577)\n",
      "(3000, 5577)\n"
     ]
    }
   ],
   "source": [
    "# print(X_tr_nn[0])\n",
    "# print(X_tr_nn[1])\n",
    "print(X_tr_nn.shape)\n",
    "print(X_te_nn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b30fd7-52e8-4fec-8803-69c7e421ef36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done importing Tensorflow Stuff\n"
     ]
    }
   ],
   "source": [
    "# Applying properly structured data to a basic FFNN\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import SparseCategoricalCrossentropy\n",
    "from keras.metrics import SparseCategoricalAccuracy\n",
    "\n",
    "print(\"Done importing Tensorflow Stuff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f025e6da-c3bf-47bb-a146-ab488730faf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make FFNN\n",
    "\n",
    "sample_length = X_tr_nn[0].shape[0]\n",
    "\n",
    "FFNN_model = Sequential([\n",
    "    Dense(sample_length, activation='relu'),\n",
    "    Dense(sample_length, activation='relu'),\n",
    "    Dense(1024, activation='relu'),\n",
    "    Dense(1024, activation='relu'),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "FFNN_model.compile(optimizer=Adam(),\n",
    "                   loss=SparseCategoricalCrossentropy(), \n",
    "                   metrics=[SparseCategoricalAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0f14b7-7541-4e8c-af62-cfb93a3dbe48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 445ms/step - loss: 2.3025 - sparse_categorical_accuracy: 0.0978\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - loss: 2.3048 - sparse_categorical_accuracy: 0.0886\n",
      "\n",
      "Test accuracy: 0.09566666930913925\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(1):\n",
    "    FFNN_model.fit(X_tr_nn, y_tr_nn)\n",
    "    test_loss, test_acc = FFNN_model.evaluate(X_te_nn, y_te_nn)\n",
    "    print(f'\\nTest accuracy: {test_acc}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
